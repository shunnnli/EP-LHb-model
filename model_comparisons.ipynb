{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison using MNIST\n",
    "\n",
    "Shun Li, 03/07/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from EPLHb import EPLHb, gd, adam\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading MNIST data\n",
    "\n",
    "train_data = datasets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "test_data = datasets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())\n",
    "\n",
    "# Loading the data\n",
    "batch_size = 100 # the size of input data took for one iteration\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data,batch_size = batch_size,shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data,batch_size = batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to be tested\n",
    "\n",
    "Different initialization scenarios\n",
    "- random initialization of every synapses\n",
    "- Dale's law initialization of every synapses\n",
    "\n",
    "Different network structure\n",
    "- LHb to DAN is all inhibitory + LHb to LHb is all excitatory (if RNN)\n",
    "- Every layer have mixed excitatory/inhibitory output\n",
    "\n",
    "Different update methods\n",
    "- Normal ADAM\n",
    "- Fixed sign ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization = ['random','dales_law']\n",
    "network_struct = ['real','mixed']\n",
    "update_methods = ['corelease','fixed_sign']\n",
    "\n",
    "LHb_network = ['MLP','RNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
    "LHb_size = 500 # number of nodes at hidden layer\n",
    "DAN_size = 10 # number of output classes discrete range [0,9]\n",
    "num_epochs = 10 # 20 # number of times which the entire dataset is passed throughout the model\n",
    "lr = 1e-3 # size of step\n",
    "\n",
    "prob_EP_to_LHb = 1\n",
    "prob_LHb_to_LHb = 1\n",
    "prob_LHb_to_DAN = 1\n",
    "\n",
    "n_networks = 20 # number of networks to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_summary, test_accuracy_summar = {}, {}\n",
    "\n",
    "for LHb in LHb_network:\n",
    "    for init in initialization:\n",
    "        for struct in network_struct:\n",
    "            for method in update_methods:\n",
    "                print('Initialization:',init,'Network:',struct,'Method:',method)\n",
    "                \n",
    "                # Initialize network-specific loss and accuracy summary\n",
    "                network_training_loss, network_test_accuracy = [], []\n",
    "\n",
    "                # Initialize network params\n",
    "                if LHb == 'MLP': rnn = False\n",
    "                else: rnn = True\n",
    "                if init == 'random': fixed_sign_init = False\n",
    "                else: fixed_sign_init = True\n",
    "                if struct == 'real': real_circuit = True\n",
    "                else: real_circuit = False\n",
    "                if method == 'corelease': fixed_sign_update = False\n",
    "                else: fixed_sign_update = True\n",
    "\n",
    "                # Train n_networks networks\n",
    "                for i in range(1,n_networks+1):\n",
    "                    # Initialize a network\n",
    "                    net = EPLHb(EP_size,LHb_size,DAN_size,\n",
    "                                rnn=rnn,fixed_sign=fixed_sign_init,real_circuit=real_circuit,\n",
    "                                prob_EP_to_LHb=prob_EP_to_LHb,prob_LHb_to_LHb=prob_LHb_to_LHb,prob_LHb_to_DAN=prob_LHb_to_DAN)\n",
    "                    initial_params = net.record_params(calc_sign=False)\n",
    "                    training_loss, test_accuracy = [], []\n",
    "                    if torch.cuda.is_available(): net.cuda()\n",
    "\n",
    "                    # Train on original data\n",
    "                    optimizer = adam(net.parameters(), lr=lr, fixed_sign=fixed_sign_update)\n",
    "                    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "                    training_loss, test_accuracy = net.train_model(num_epochs,train_loader,optimizer,\n",
    "                                                    test_loader=test_loader,print_epoch=False,loss='CrossEntropyLoss')\n",
    "                    training_loss.extend(training_loss)\n",
    "                    test_accuracy.extend(test_accuracy)\n",
    "\n",
    "                    # Train on flipped data\n",
    "                    # optimizer = adam(net.parameters(), lr=lr, fixed_sign=fixed_sign_update)\n",
    "                    # training_loss = net.train_model(num_epochs,flip_loader,optimizer,print_epoch=False)\n",
    "                    # net_training_loss.extend(training_loss)\n",
    "\n",
    "                    network_training_loss.append(training_loss)\n",
    "                    network_test_accuracy.append(test_accuracy)\n",
    "                    print('Finished training co-release network %d/%d' %(i,n_networks))\n",
    "\n",
    "                # Convert list to numpy array\n",
    "                network_training_loss = np.array(network_training_loss)\n",
    "                network_test_accuracy = np.array(network_test_accuracy)\n",
    "\n",
    "                # Store name and stats of network to summary\n",
    "                network_name = LHb+'_'+init+'_'+struct+'_'+method\n",
    "                training_loss_summary[network_name] = network_training_loss\n",
    "                test_accuracy_summar[network_name] = network_test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss\n",
    "# mean_cr_loss = np.mean(cr_training_loss_summary,axis=0)\n",
    "# sem_cr_loss = stats.sem(cr_training_loss_summary)\n",
    "# mean_fs_loss = np.mean(fs_training_loss_summary,axis=0)\n",
    "# sem_fs_loss = stats.sem(fs_training_loss_summary)\n",
    "# mean_fs_posneg_loss = np.mean(fs_posneg_training_loss_summary,axis=0)\n",
    "# sem_fs_posneg_loss = stats.sem(fs_posneg_training_loss_summary)\n",
    "\n",
    "# # Test accuracy\n",
    "# mean_cr_accuracy = np.mean(cr_test_accuracy_summary,axis=0)\n",
    "# sem_cr_accuracy = stats.sem(cr_test_accuracy_summary)\n",
    "# mean_fs_accuracy = np.mean(fs_test_accuracy_summary,axis=0)\n",
    "# sem_fs_accuracy = stats.sem(fs_test_accuracy_summary)\n",
    "# mean_fs_posneg_accuracy = np.mean(fs_posneg_test_accuracy_summary,axis=0)\n",
    "# sem_fs_posneg_accuracy = stats.sem(fs_posneg_test_accuracy_summary)\n",
    "\n",
    "# # Plot\n",
    "# fig, axs = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "# # Plot loss\n",
    "# x = np.linspace(1,mean_cr_loss.shape[0],num=mean_cr_loss.shape[0],dtype='int32')\n",
    "# axs[0].plot(mean_cr_loss, label='Co-release')\n",
    "# axs[0].fill_between(x,mean_cr_loss+sem_cr_loss,mean_cr_loss-sem_cr_loss,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_loss, label='Fixed sign')\n",
    "# axs[0].fill_between(x,mean_fs_loss+sem_fs_loss,mean_fs_loss-sem_fs_loss,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_posneg_loss, label='Fixed sign without neg output')\n",
    "# axs[0].fill_between(x,mean_fs_posneg_loss+sem_fs_posneg_loss,mean_fs_posneg_loss-sem_fs_posneg_loss,alpha=0.2)\n",
    "\n",
    "# axs[0].set_xlabel('Trianing epochs')\n",
    "# axs[0].set_ylabel('Training loss')\n",
    "# axs[0].legend()\n",
    "\n",
    "# # Plot accuracy\n",
    "# x = np.linspace(1,mean_cr_accuracy.shape[0],num=mean_cr_accuracy.shape[0],dtype='int32')\n",
    "# axs[0].plot(mean_cr_accuracy, label='Co-release')\n",
    "# axs[0].fill_between(x,mean_cr_accuracy+sem_cr_accuracy,mean_cr_accuracy-sem_cr_accuracy,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_accuracy, label='Fixed sign')\n",
    "# axs[0].fill_between(x,mean_fs_accuracy+sem_fs_accuracy,mean_fs_accuracy-sem_fs_accuracy,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_posneg_accuracy, label='Fixed sign without neg output')\n",
    "# axs[0].fill_between(x,mean_fs_posneg_accuracy+sem_fs_posneg_accuracy,mean_fs_posneg_accuracy-sem_fs_posneg_accuracy,alpha=0.2)\n",
    "\n",
    "# axs[0].set_xlabel('Trianing epochs')\n",
    "# axs[0].set_ylabel('Training loss')\n",
    "# axs[0].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplhbmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
