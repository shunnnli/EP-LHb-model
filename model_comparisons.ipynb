{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison using MNIST\n",
    "\n",
    "Shun Li, 03/07/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from EPLHb import EPLHb, gd, adam\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading MNIST data\n",
    "\n",
    "train_data = datasets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "test_data = datasets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())\n",
    "\n",
    "# Loading the data\n",
    "batch_size = 100 # the size of input data took for one iteration\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data,batch_size = batch_size,shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data,batch_size = batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to be tested\n",
    "\n",
    "Different initialization scenarios\n",
    "- random initialization of every synapses\n",
    "- Dale's law initialization of every synapses\n",
    "\n",
    "Different network structure\n",
    "- LHb to DAN is all inhibitory + LHb to LHb is all excitatory (if RNN)\n",
    "- Every layer have mixed excitatory/inhibitory output\n",
    "\n",
    "Different update methods\n",
    "- Normal ADAM\n",
    "- Fixed sign ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization = ['random','dales_law']\n",
    "network_struct = ['real','mixed']\n",
    "update_methods = ['corelease','fixed_sign']\n",
    "\n",
    "LHb_network = ['MLP','RNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
    "LHb_size = 500 # number of nodes at hidden layer\n",
    "DAN_size = 10 # number of output classes discrete range [0,9]\n",
    "num_epochs = 10 # 20 # number of times which the entire dataset is passed throughout the model\n",
    "lr = 1e-3 # size of step\n",
    "\n",
    "prob_EP_to_LHb = 1\n",
    "prob_LHb_to_LHb = 1\n",
    "prob_LHb_to_DAN = 1\n",
    "\n",
    "n_networks = 2 # number of networks to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHb:  MLP ; Initialization: random ; Network: real ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  MLP ; Initialization: random ; Network: real ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  MLP ; Initialization: random ; Network: mixed ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  MLP ; Initialization: random ; Network: mixed ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  MLP ; Initialization: dales_law ; Network: real ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  MLP ; Initialization: dales_law ; Network: real ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  MLP ; Initialization: dales_law ; Network: mixed ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  MLP ; Initialization: dales_law ; Network: mixed ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: random ; Network: real ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: random ; Network: real ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: random ; Network: mixed ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: random ; Network: mixed ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: dales_law ; Network: real ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: dales_law ; Network: real ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: dales_law ; Network: mixed ; Method: corelease\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n",
      "LHb:  RNN ; Initialization: dales_law ; Network: mixed ; Method: fixed_sign\n",
      "Finished training co-release network 1/2\n",
      "Finished training co-release network 2/2\n"
     ]
    }
   ],
   "source": [
    "training_loss_summary, test_accuracy_summar = {}, {}\n",
    "\n",
    "for LHb in LHb_network:\n",
    "    for init in initialization:\n",
    "        for struct in network_struct:\n",
    "            for method in update_methods:\n",
    "                print('LHb: ',LHb, '; Initialization:',init,'; Network:',struct,'; Method:',method)\n",
    "                \n",
    "                # Initialize network-specific loss and accuracy summary\n",
    "                network_training_loss, network_test_accuracy = [], []\n",
    "\n",
    "                # Initialize network params\n",
    "                if LHb == 'MLP': rnn = False\n",
    "                else: rnn = True\n",
    "                if init == 'random': fixed_sign_init = False\n",
    "                else: fixed_sign_init = True\n",
    "                if struct == 'real': real_circuit = True\n",
    "                else: real_circuit = False\n",
    "                if method == 'corelease': fixed_sign_update = False\n",
    "                else: fixed_sign_update = True\n",
    "\n",
    "                # Train n_networks networks\n",
    "                for i in range(1,n_networks+1):\n",
    "                    # Initialize a network\n",
    "                    net = EPLHb(EP_size,LHb_size,DAN_size,\n",
    "                                rnn=rnn,fixed_sign=fixed_sign_init,real_circuit=real_circuit,\n",
    "                                prob_EP_to_LHb=prob_EP_to_LHb,prob_LHb_to_LHb=prob_LHb_to_LHb,prob_LHb_to_DAN=prob_LHb_to_DAN)\n",
    "                    initial_params = net.record_params(calc_sign=False)\n",
    "                    training_loss, test_accuracy = [], []\n",
    "                    if torch.cuda.is_available(): net.cuda()\n",
    "\n",
    "                    # Train on original data\n",
    "                    optimizer = adam(net.parameters(), lr=lr, fixed_sign=fixed_sign_update)\n",
    "                    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "                    training_loss, test_accuracy = net.train_model(num_epochs,train_loader,optimizer,\n",
    "                                                    test_loader=test_loader,print_epoch=False,loss='CrossEntropyLoss')\n",
    "                    training_loss.extend(training_loss)\n",
    "                    test_accuracy.extend(test_accuracy)\n",
    "\n",
    "                    # Train on flipped data\n",
    "                    # optimizer = adam(net.parameters(), lr=lr, fixed_sign=fixed_sign_update)\n",
    "                    # training_loss = net.train_model(num_epochs,flip_loader,optimizer,print_epoch=False)\n",
    "                    # net_training_loss.extend(training_loss)\n",
    "\n",
    "                    network_training_loss.append(training_loss)\n",
    "                    network_test_accuracy.append(test_accuracy)\n",
    "                    print('Finished training network %d/%d' %(i,n_networks))\n",
    "\n",
    "                # Convert list to numpy array\n",
    "                network_training_loss = np.array(network_training_loss)\n",
    "                network_test_accuracy = np.array(network_test_accuracy)\n",
    "\n",
    "                # Store name and stats of network to summary\n",
    "                network_name = LHb+'_'+init+'_'+struct+'_'+method\n",
    "                training_loss_summary[network_name] = network_training_loss\n",
    "                test_accuracy_summar[network_name] = network_test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_random_real_corelease\n",
      "Training Loss: [2.296679   2.2139473  2.1373115  ... 0.8231911  0.8318355  0.80935943]\n",
      "Test Accuracy: [17.4      89.399994 90.74     91.61     92.285    92.565    92.955\n",
      " 93.244995 93.73     94.195    94.229996 94.64     94.96     95.085\n",
      " 95.325    95.325    95.405    95.66     95.64     95.835    96.05\n",
      " 96.195    96.119995 96.175    96.46     96.44     96.565    96.7\n",
      " 96.770004 96.68     96.88     96.880005 96.97     96.92     96.979996\n",
      " 97.       97.185    97.134995 97.155    97.155    97.16     97.275\n",
      " 97.25     97.295    97.350006 97.415    97.369995 97.41     97.405\n",
      " 97.475006 97.405    97.57     97.42     97.47     97.545    97.54\n",
      " 97.61     97.595    97.485    97.53     17.4      89.399994 90.74\n",
      " 91.61     92.285    92.565    92.955    93.244995 93.73     94.195\n",
      " 94.229996 94.64     94.96     95.085    95.325    95.325    95.405\n",
      " 95.66     95.64     95.835    96.05     96.195    96.119995 96.175\n",
      " 96.46     96.44     96.565    96.7      96.770004 96.68     96.88\n",
      " 96.880005 96.97     96.92     96.979996 97.       97.185    97.134995\n",
      " 97.155    97.155    97.16     97.275    97.25     97.295    97.350006\n",
      " 97.415    97.369995 97.41     97.405    97.475006 97.405    97.57\n",
      " 97.42     97.47     97.545    97.54     97.61     97.595    97.485\n",
      " 97.53    ]\n",
      "MLP_random_real_fixed_sign\n",
      "Training Loss: [2.3155031 2.2202227 2.1451802 ... 0.8452925 0.8175875 0.8530893]\n",
      "Test Accuracy: [35.095    89.05     90.39     91.34     91.229996 92.19     92.41\n",
      " 92.945    93.02     93.23     93.979996 93.93     94.235    94.64\n",
      " 94.515    94.630005 95.130005 95.255005 95.16     95.25     95.575\n",
      " 95.63     95.745    95.925    96.024994 96.125    96.21     96.345\n",
      " 96.31     96.35     96.515    96.61     96.615005 96.57     96.79\n",
      " 96.765    96.845    96.854996 96.82     96.94501  96.835    96.965\n",
      " 96.865005 97.065    97.055    97.115    97.155    97.195    97.215\n",
      " 97.295    97.259995 97.305    97.29     97.399994 97.415    97.380005\n",
      " 97.41     97.335    97.345    97.345    35.095    89.05     90.39\n",
      " 91.34     91.229996 92.19     92.41     92.945    93.02     93.23\n",
      " 93.979996 93.93     94.235    94.64     94.515    94.630005 95.130005\n",
      " 95.255005 95.16     95.25     95.575    95.63     95.745    95.925\n",
      " 96.024994 96.125    96.21     96.345    96.31     96.35     96.515\n",
      " 96.61     96.615005 96.57     96.79     96.765    96.845    96.854996\n",
      " 96.82     96.94501  96.835    96.965    96.865005 97.065    97.055\n",
      " 97.115    97.155    97.195    97.215    97.295    97.259995 97.305\n",
      " 97.29     97.399994 97.415    97.380005 97.41     97.335    97.345\n",
      " 97.345   ]\n",
      "MLP_random_mixed_corelease\n",
      "Training Loss: [2.3134315  2.1631498  2.0575898  ... 0.82750964 0.81552446 0.8216909 ]\n",
      "Test Accuracy: [27.98     89.825    90.675    91.545    92.130005 92.345    93.100006\n",
      " 93.395004 93.645    94.21     94.43     94.635    94.625    95.240005\n",
      " 95.15     95.55     95.795    95.92     95.935    96.03     96.165\n",
      " 96.48     96.354996 96.64     96.76     96.76     96.775    96.735\n",
      " 96.925    96.975    96.869995 97.085    97.085    97.185    97.14\n",
      " 97.145004 97.305    97.285    97.38     97.415    97.3      97.33\n",
      " 97.215    97.479996 97.59     97.49     97.600006 97.495    97.615\n",
      " 97.535    97.575    97.67     97.64     97.71     97.66     97.729996\n",
      " 97.79     97.729996 97.69     97.715    27.98     89.825    90.675\n",
      " 91.545    92.130005 92.345    93.100006 93.395004 93.645    94.21\n",
      " 94.43     94.635    94.625    95.240005 95.15     95.55     95.795\n",
      " 95.92     95.935    96.03     96.165    96.48     96.354996 96.64\n",
      " 96.76     96.76     96.775    96.735    96.925    96.975    96.869995\n",
      " 97.085    97.085    97.185    97.14     97.145004 97.305    97.285\n",
      " 97.38     97.415    97.3      97.33     97.215    97.479996 97.59\n",
      " 97.49     97.600006 97.495    97.615    97.535    97.575    97.67\n",
      " 97.64     97.71     97.66     97.729996 97.79     97.729996 97.69\n",
      " 97.715   ]\n",
      "MLP_random_mixed_fixed_sign\n",
      "Training Loss: [2.310425   2.1436162  2.077744   ... 0.84529877 0.8379206  0.8256391 ]\n",
      "Test Accuracy: [26.865    89.555    90.43     91.34     91.825    91.875    92.415\n",
      " 92.655    92.990005 93.345    93.915    93.74     94.035    94.24\n",
      " 94.72     94.630005 94.95     95.205    95.380005 95.585    95.735\n",
      " 95.615005 96.01     96.145004 96.055    96.38     96.350006 96.2\n",
      " 96.58     96.7      96.565    96.740005 96.82     96.79     96.744995\n",
      " 96.94     96.875    97.06     96.97     97.005    96.994995 97.16499\n",
      " 97.165    97.27     97.235    97.32     97.22     97.14     97.240005\n",
      " 97.285    97.35     97.479996 97.435    97.425    97.37     97.585\n",
      " 97.29     97.600006 97.479996 97.524994 26.865    89.555    90.43\n",
      " 91.34     91.825    91.875    92.415    92.655    92.990005 93.345\n",
      " 93.915    93.74     94.035    94.24     94.72     94.630005 94.95\n",
      " 95.205    95.380005 95.585    95.735    95.615005 96.01     96.145004\n",
      " 96.055    96.38     96.350006 96.2      96.58     96.7      96.565\n",
      " 96.740005 96.82     96.79     96.744995 96.94     96.875    97.06\n",
      " 96.97     97.005    96.994995 97.16499  97.165    97.27     97.235\n",
      " 97.32     97.22     97.14     97.240005 97.285    97.35     97.479996\n",
      " 97.435    97.425    97.37     97.585    97.29     97.600006 97.479996\n",
      " 97.524994]\n",
      "MLP_dales_law_real_corelease\n",
      "Training Loss: [2.3071594  2.2612321  2.2455454  ... 0.81960213 0.8185359  0.8014649 ]\n",
      "Test Accuracy: [28.74     89.07     90.735    91.295    92.315    92.575    93.115\n",
      " 93.475006 93.68     94.15     94.41     94.520004 94.84     95.07\n",
      " 95.24     95.380005 95.325    95.595    95.83501  95.915    96.130005\n",
      " 96.215    96.175    96.32     96.475006 96.58     96.59     96.729996\n",
      " 96.604996 96.71     96.83     96.815    96.935    97.020004 97.025\n",
      " 96.975    97.06     97.21     97.215    97.16     97.185    97.28\n",
      " 97.28999  97.275    97.375    97.31     97.355    97.41     97.41\n",
      " 97.52     97.350006 97.520004 97.479996 97.49     97.5      97.615005\n",
      " 97.485    97.56     97.630005 97.545    28.74     89.07     90.735\n",
      " 91.295    92.315    92.575    93.115    93.475006 93.68     94.15\n",
      " 94.41     94.520004 94.84     95.07     95.24     95.380005 95.325\n",
      " 95.595    95.83501  95.915    96.130005 96.215    96.175    96.32\n",
      " 96.475006 96.58     96.59     96.729996 96.604996 96.71     96.83\n",
      " 96.815    96.935    97.020004 97.025    96.975    97.06     97.21\n",
      " 97.215    97.16     97.185    97.28     97.28999  97.275    97.375\n",
      " 97.31     97.355    97.41     97.41     97.52     97.350006 97.520004\n",
      " 97.479996 97.49     97.5      97.615005 97.485    97.56     97.630005\n",
      " 97.545   ]\n",
      "MLP_dales_law_real_fixed_sign\n",
      "Training Loss: [2.2993746  2.2640224  2.2366912  ... 0.8934687  0.91033375 0.8319271 ]\n",
      "Test Accuracy: [27.425    87.845    89.94501  90.9      90.869995 91.395004 91.635\n",
      " 91.71     92.17     92.555    92.66499  93.020004 93.155    93.375\n",
      " 93.445    93.435    93.899994 93.81     93.935    94.155    94.425\n",
      " 94.64     94.600006 94.815    94.84     94.96001  94.94     95.115005\n",
      " 95.17     95.365005 95.53     95.46     95.39     95.69     95.79\n",
      " 95.72     95.91499  95.9      95.94     95.945    96.005    95.955\n",
      " 96.18     96.274994 96.215    96.11     96.275    96.275    96.490005\n",
      " 96.555    96.53     96.445    96.229996 96.65     96.634995 96.619995\n",
      " 96.645    96.715    96.625    96.71     27.425    87.845    89.94501\n",
      " 90.9      90.869995 91.395004 91.635    91.71     92.17     92.555\n",
      " 92.66499  93.020004 93.155    93.375    93.445    93.435    93.899994\n",
      " 93.81     93.935    94.155    94.425    94.64     94.600006 94.815\n",
      " 94.84     94.96001  94.94     95.115005 95.17     95.365005 95.53\n",
      " 95.46     95.39     95.69     95.79     95.72     95.91499  95.9\n",
      " 95.94     95.945    96.005    95.955    96.18     96.274994 96.215\n",
      " 96.11     96.275    96.275    96.490005 96.555    96.53     96.445\n",
      " 96.229996 96.65     96.634995 96.619995 96.645    96.715    96.625\n",
      " 96.71    ]\n",
      "MLP_dales_law_mixed_corelease\n",
      "Training Loss: [2.309907   2.2201886  2.1380796  ... 0.8309039  0.81469345 0.824438  ]\n",
      "Test Accuracy: [31.055    89.14     90.95     91.53     92.43     92.555    93.32\n",
      " 93.575    93.935    94.085    94.755005 94.875    94.91     95.39\n",
      " 95.475006 95.685    95.8      96.035    96.05     96.25     96.41499\n",
      " 96.32     96.525    96.759995 96.83501  96.925    96.74     96.95\n",
      " 97.025    97.020004 96.93     97.115    97.255005 97.255    97.38\n",
      " 97.195    97.22     97.4      97.375    97.425    97.490005 97.369995\n",
      " 97.33     97.595    97.490005 97.44501  97.435    97.525    97.56\n",
      " 97.61     97.57     97.490005 97.695    97.685    97.585    97.695\n",
      " 97.7      97.735    97.79     97.825    31.055    89.14     90.95\n",
      " 91.53     92.43     92.555    93.32     93.575    93.935    94.085\n",
      " 94.755005 94.875    94.91     95.39     95.475006 95.685    95.8\n",
      " 96.035    96.05     96.25     96.41499  96.32     96.525    96.759995\n",
      " 96.83501  96.925    96.74     96.95     97.025    97.020004 96.93\n",
      " 97.115    97.255005 97.255    97.38     97.195    97.22     97.4\n",
      " 97.375    97.425    97.490005 97.369995 97.33     97.595    97.490005\n",
      " 97.44501  97.435    97.525    97.56     97.61     97.57     97.490005\n",
      " 97.695    97.685    97.585    97.695    97.7      97.735    97.79\n",
      " 97.825   ]\n",
      "MLP_dales_law_mixed_fixed_sign\n",
      "Training Loss: [2.3034792  2.221778   2.136634   ... 0.8221589  0.8542678  0.82238173]\n",
      "Test Accuracy: [34.510002 88.86     90.32     90.729996 91.67     92.075    92.425\n",
      " 92.899994 92.8      93.04     93.520004 93.685    93.905    94.229996\n",
      " 94.369995 94.695    94.8      94.755005 94.98     95.04     95.24\n",
      " 95.47     95.46001  95.565    95.915    95.925    95.94     96.05\n",
      " 96.2      96.244995 96.369995 96.399994 96.384995 96.5      96.5\n",
      " 96.425    96.61     96.735    96.725006 96.79     96.83     96.85\n",
      " 96.95     96.87     96.86     97.035    97.08     96.92     96.895004\n",
      " 97.075    97.025    97.09     97.100006 97.       97.21     97.125\n",
      " 97.28999  97.149994 97.32     97.315    34.510002 88.86     90.32\n",
      " 90.729996 91.67     92.075    92.425    92.899994 92.8      93.04\n",
      " 93.520004 93.685    93.905    94.229996 94.369995 94.695    94.8\n",
      " 94.755005 94.98     95.04     95.24     95.47     95.46001  95.565\n",
      " 95.915    95.925    95.94     96.05     96.2      96.244995 96.369995\n",
      " 96.399994 96.384995 96.5      96.5      96.425    96.61     96.735\n",
      " 96.725006 96.79     96.83     96.85     96.95     96.87     96.86\n",
      " 97.035    97.08     96.92     96.895004 97.075    97.025    97.09\n",
      " 97.100006 97.       97.21     97.125    97.28999  97.149994 97.32\n",
      " 97.315   ]\n",
      "RNN_random_real_corelease\n",
      "Training Loss: [2.3039818  2.2289732  2.1288524  ... 0.82657754 0.82334745 0.8123017 ]\n",
      "Test Accuracy: [37.204998 89.42     90.66     91.615005 92.085    92.565    93.16\n",
      " 93.555    93.744995 93.945    94.41499  94.520004 94.69     95.245\n",
      " 95.275    95.48     95.605    95.72     95.65     95.85     95.915\n",
      " 96.33     96.395    96.45     96.47     96.625    96.774994 96.744995\n",
      " 96.744995 96.785    96.895004 97.01     96.93     97.085    96.97\n",
      " 97.145004 97.08     97.100006 97.25     97.255    97.270004 97.255\n",
      " 97.375    97.274994 97.41     97.315    97.295    97.354996 97.355\n",
      " 97.46     97.53     97.56     97.46     97.479996 97.545    97.61\n",
      " 97.615    97.600006 97.595    97.615    37.204998 89.42     90.66\n",
      " 91.615005 92.085    92.565    93.16     93.555    93.744995 93.945\n",
      " 94.41499  94.520004 94.69     95.245    95.275    95.48     95.605\n",
      " 95.72     95.65     95.85     95.915    96.33     96.395    96.45\n",
      " 96.47     96.625    96.774994 96.744995 96.744995 96.785    96.895004\n",
      " 97.01     96.93     97.085    96.97     97.145004 97.08     97.100006\n",
      " 97.25     97.255    97.270004 97.255    97.375    97.274994 97.41\n",
      " 97.315    97.295    97.354996 97.355    97.46     97.53     97.56\n",
      " 97.46     97.479996 97.545    97.61     97.615    97.600006 97.595\n",
      " 97.615   ]\n",
      "RNN_random_real_fixed_sign\n",
      "Training Loss: [2.3008335  2.225924   2.1366112  ... 0.87286675 0.83285457 0.8125698 ]\n",
      "Test Accuracy: [34.025    89.39     90.05499  91.215    91.735    92.375    92.774994\n",
      " 92.604996 93.285    93.515    94.18     94.024994 94.265    94.44\n",
      " 94.78     94.985    94.91     95.16     95.350006 95.515    95.695\n",
      " 95.56     95.835    96.020004 96.025    96.315    96.265    96.28\n",
      " 96.274994 96.415    96.47     96.515    96.61     96.795    96.75\n",
      " 96.765    96.715    97.015    96.935    96.91     97.024994 97.095\n",
      " 96.94     97.09     97.06     97.270004 97.11     97.2      97.22\n",
      " 97.354996 97.285    97.32     97.29     97.32     97.295    97.41\n",
      " 97.41     97.4      97.375    97.455    34.025    89.39     90.05499\n",
      " 91.215    91.735    92.375    92.774994 92.604996 93.285    93.515\n",
      " 94.18     94.024994 94.265    94.44     94.78     94.985    94.91\n",
      " 95.16     95.350006 95.515    95.695    95.56     95.835    96.020004\n",
      " 96.025    96.315    96.265    96.28     96.274994 96.415    96.47\n",
      " 96.515    96.61     96.795    96.75     96.765    96.715    97.015\n",
      " 96.935    96.91     97.024994 97.095    96.94     97.09     97.06\n",
      " 97.270004 97.11     97.2      97.22     97.354996 97.285    97.32\n",
      " 97.29     97.32     97.295    97.41     97.41     97.4      97.375\n",
      " 97.455   ]\n",
      "RNN_random_mixed_corelease\n",
      "Training Loss: [2.3015249  2.1482105  2.0338063  ... 0.82201517 0.8074926  0.8086009 ]\n",
      "Test Accuracy: [29.075    89.78     91.075    91.835    92.14     92.615005 93.229996\n",
      " 93.55     94.119995 94.185    94.59     94.615    94.995    95.455\n",
      " 95.600006 95.46001  95.75     95.994995 96.045    96.244995 96.255005\n",
      " 96.47     96.525    96.525    96.55499  96.71001  96.76     96.990005\n",
      " 97.005005 96.92     97.104996 97.155    97.16     97.07     97.185\n",
      " 97.255    97.335    97.21     97.415    97.354996 97.42     97.505\n",
      " 97.44     97.58     97.515    97.58     97.509995 97.47     97.66499\n",
      " 97.76     97.625    97.535    97.68     97.57     97.655    97.785\n",
      " 97.64     97.795    97.795    97.880005 29.075    89.78     91.075\n",
      " 91.835    92.14     92.615005 93.229996 93.55     94.119995 94.185\n",
      " 94.59     94.615    94.995    95.455    95.600006 95.46001  95.75\n",
      " 95.994995 96.045    96.244995 96.255005 96.47     96.525    96.525\n",
      " 96.55499  96.71001  96.76     96.990005 97.005005 96.92     97.104996\n",
      " 97.155    97.16     97.07     97.185    97.255    97.335    97.21\n",
      " 97.415    97.354996 97.42     97.505    97.44     97.58     97.515\n",
      " 97.58     97.509995 97.47     97.66499  97.76     97.625    97.535\n",
      " 97.68     97.57     97.655    97.785    97.64     97.795    97.795\n",
      " 97.880005]\n",
      "RNN_random_mixed_fixed_sign\n",
      "Training Loss: [2.2853906  2.1635501  2.031364   ... 0.8289807  0.8227936  0.86036575]\n",
      "Test Accuracy: [33.3      89.735    90.765    91.65     91.75     92.255005 92.535\n",
      " 93.005    93.43     93.675    93.884995 94.115    94.525    94.775\n",
      " 94.880005 95.22     95.244995 95.354996 95.58     95.85     95.91\n",
      " 95.925    96.       96.305    96.35     96.490005 96.56     96.545\n",
      " 96.595    96.59     96.75     96.785    96.895004 96.955    96.75\n",
      " 97.095    97.04     96.925    97.09     97.015    97.195    97.32\n",
      " 97.229996 97.31     97.259995 97.345    97.375    97.34     97.405\n",
      " 97.41     97.369995 97.57     97.565    97.505    97.520004 97.520004\n",
      " 97.58     97.625    97.625    97.61     33.3      89.735    90.765\n",
      " 91.65     91.75     92.255005 92.535    93.005    93.43     93.675\n",
      " 93.884995 94.115    94.525    94.775    94.880005 95.22     95.244995\n",
      " 95.354996 95.58     95.85     95.91     95.925    96.       96.305\n",
      " 96.35     96.490005 96.56     96.545    96.595    96.59     96.75\n",
      " 96.785    96.895004 96.955    96.75     97.095    97.04     96.925\n",
      " 97.09     97.015    97.195    97.32     97.229996 97.31     97.259995\n",
      " 97.345    97.375    97.34     97.405    97.41     97.369995 97.57\n",
      " 97.565    97.505    97.520004 97.520004 97.58     97.625    97.625\n",
      " 97.61    ]\n",
      "RNN_dales_law_real_corelease\n",
      "Training Loss: [2.3008833  2.2758384  2.2566214  ... 0.8176098  0.82054806 0.83208287]\n",
      "Test Accuracy: [15.030001 88.68     90.46     91.395004 92.145004 92.675    93.2\n",
      " 93.33     93.869995 94.21001  94.385    94.585    94.845    94.91\n",
      " 95.31     95.31     95.61     95.75     95.86     96.005005 96.1\n",
      " 96.235    96.26     96.345    96.57     96.565    96.69     96.84\n",
      " 96.79     96.815    96.850006 96.97     97.04     97.065    97.095\n",
      " 97.09     97.195    97.229996 97.229996 97.175    97.325    97.33\n",
      " 97.17     97.415    97.41     97.380005 97.33     97.33     97.235\n",
      " 97.55     97.465    97.47     97.54     97.479996 97.595    97.555\n",
      " 97.66     97.59     97.619995 97.625    15.030001 88.68     90.46\n",
      " 91.395004 92.145004 92.675    93.2      93.33     93.869995 94.21001\n",
      " 94.385    94.585    94.845    94.91     95.31     95.31     95.61\n",
      " 95.75     95.86     96.005005 96.1      96.235    96.26     96.345\n",
      " 96.57     96.565    96.69     96.84     96.79     96.815    96.850006\n",
      " 96.97     97.04     97.065    97.095    97.09     97.195    97.229996\n",
      " 97.229996 97.175    97.325    97.33     97.17     97.415    97.41\n",
      " 97.380005 97.33     97.33     97.235    97.55     97.465    97.47\n",
      " 97.54     97.479996 97.595    97.555    97.66     97.59     97.619995\n",
      " 97.625   ]\n",
      "RNN_dales_law_real_fixed_sign\n",
      "Training Loss: [2.3031366 2.2687945 2.2239234 ... 0.9137548 0.9199116 0.9691659]\n",
      "Test Accuracy: [30.425    83.89     85.205    86.2      86.455    86.755005 87.41\n",
      " 87.380005 87.78999  88.095    88.45     88.69     89.03999  89.04\n",
      " 89.175    89.045    89.715    89.595    89.765    89.91     89.865005\n",
      " 90.16     90.16499  90.39     90.39     90.495    90.58501  90.69\n",
      " 90.7      90.880005 90.97     91.095    91.06     91.13     91.08\n",
      " 91.29     91.22     91.41499  91.375    91.274994 91.51     91.57\n",
      " 91.625    91.555    91.634995 91.600006 91.64     91.619995 91.66\n",
      " 91.774994 91.815    91.805    91.935    91.89     92.04     92.025\n",
      " 91.985    92.015    91.994995 92.065    30.425    83.89     85.205\n",
      " 86.2      86.455    86.755005 87.41     87.380005 87.78999  88.095\n",
      " 88.45     88.69     89.03999  89.04     89.175    89.045    89.715\n",
      " 89.595    89.765    89.91     89.865005 90.16     90.16499  90.39\n",
      " 90.39     90.495    90.58501  90.69     90.7      90.880005 90.97\n",
      " 91.095    91.06     91.13     91.08     91.29     91.22     91.41499\n",
      " 91.375    91.274994 91.51     91.57     91.625    91.555    91.634995\n",
      " 91.600006 91.64     91.619995 91.66     91.774994 91.815    91.805\n",
      " 91.935    91.89     92.04     92.025    91.985    92.015    91.994995\n",
      " 92.065   ]\n",
      "RNN_dales_law_mixed_corelease\n",
      "Training Loss: [2.3118382  2.2102485  2.1394792  ... 0.80451596 0.8320032  0.8107836 ]\n",
      "Test Accuracy: [31.07     88.71     90.785    91.97     92.45     93.015    93.41499\n",
      " 93.925    94.095    94.46     94.705    94.755005 95.240005 95.380005\n",
      " 95.335    95.795    95.695    95.865    96.16     96.245    96.435\n",
      " 96.384995 96.71001  96.555    96.8      96.845    96.89     96.955\n",
      " 97.065    97.04     97.005005 97.       97.155    97.215    97.259995\n",
      " 97.315    97.32     97.36     97.465    97.45     97.41     97.505005\n",
      " 97.5      97.604996 97.435    97.475    97.545    97.56     97.604996\n",
      " 97.66     97.759995 97.55499  97.65     97.65     97.67     97.725006\n",
      " 97.759995 97.845    97.7      97.83     31.07     88.71     90.785\n",
      " 91.97     92.45     93.015    93.41499  93.925    94.095    94.46\n",
      " 94.705    94.755005 95.240005 95.380005 95.335    95.795    95.695\n",
      " 95.865    96.16     96.245    96.435    96.384995 96.71001  96.555\n",
      " 96.8      96.845    96.89     96.955    97.065    97.04     97.005005\n",
      " 97.       97.155    97.215    97.259995 97.315    97.32     97.36\n",
      " 97.465    97.45     97.41     97.505005 97.5      97.604996 97.435\n",
      " 97.475    97.545    97.56     97.604996 97.66     97.759995 97.55499\n",
      " 97.65     97.65     97.67     97.725006 97.759995 97.845    97.7\n",
      " 97.83    ]\n",
      "RNN_dales_law_mixed_fixed_sign\n",
      "Training Loss: [2.3034482  2.198547   2.135447   ... 0.8703925  0.82434934 0.84257925]\n",
      "Test Accuracy: [29.505001 89.14     90.21     90.744995 91.615005 91.545    92.369995\n",
      " 92.64     93.035    93.44     93.565    93.725    94.14     94.28\n",
      " 94.395004 94.135    94.5      94.71     95.03     95.255005 95.32\n",
      " 95.305    95.545    95.740005 95.6      95.740005 95.795    95.975\n",
      " 95.850006 96.055    96.175    96.235    96.28     96.28     96.43\n",
      " 96.615    96.675    96.55     96.695    96.72     96.635    96.785\n",
      " 96.69501  96.729996 96.685    96.69     96.8      96.935    96.85\n",
      " 96.915    96.985    96.92     97.075    96.955    97.135    97.195\n",
      " 97.104996 97.095    97.155    97.18     29.505001 89.14     90.21\n",
      " 90.744995 91.615005 91.545    92.369995 92.64     93.035    93.44\n",
      " 93.565    93.725    94.14     94.28     94.395004 94.135    94.5\n",
      " 94.71     95.03     95.255005 95.32     95.305    95.545    95.740005\n",
      " 95.6      95.740005 95.795    95.975    95.850006 96.055    96.175\n",
      " 96.235    96.28     96.28     96.43     96.615    96.675    96.55\n",
      " 96.695    96.72     96.635    96.785    96.69501  96.729996 96.685\n",
      " 96.69     96.8      96.935    96.85     96.915    96.985    96.92\n",
      " 97.075    96.955    97.135    97.195    97.104996 97.095    97.155\n",
      " 97.18    ]\n"
     ]
    }
   ],
   "source": [
    "for network in training_loss_summary:\n",
    "    print(network)\n",
    "    print('Training Loss:', np.mean(training_loss_summary[network],axis=0))\n",
    "    print('Test Accuracy:', np.mean(test_accuracy_summar[network],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss\n",
    "# mean_cr_loss = np.mean(cr_training_loss_summary,axis=0)\n",
    "# sem_cr_loss = stats.sem(cr_training_loss_summary)\n",
    "# mean_fs_loss = np.mean(fs_training_loss_summary,axis=0)\n",
    "# sem_fs_loss = stats.sem(fs_training_loss_summary)\n",
    "# mean_fs_posneg_loss = np.mean(fs_posneg_training_loss_summary,axis=0)\n",
    "# sem_fs_posneg_loss = stats.sem(fs_posneg_training_loss_summary)\n",
    "\n",
    "# # Test accuracy\n",
    "# mean_cr_accuracy = np.mean(cr_test_accuracy_summary,axis=0)\n",
    "# sem_cr_accuracy = stats.sem(cr_test_accuracy_summary)\n",
    "# mean_fs_accuracy = np.mean(fs_test_accuracy_summary,axis=0)\n",
    "# sem_fs_accuracy = stats.sem(fs_test_accuracy_summary)\n",
    "# mean_fs_posneg_accuracy = np.mean(fs_posneg_test_accuracy_summary,axis=0)\n",
    "# sem_fs_posneg_accuracy = stats.sem(fs_posneg_test_accuracy_summary)\n",
    "\n",
    "# # Plot\n",
    "# fig, axs = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "# # Plot loss\n",
    "# x = np.linspace(1,mean_cr_loss.shape[0],num=mean_cr_loss.shape[0],dtype='int32')\n",
    "# axs[0].plot(mean_cr_loss, label='Co-release')\n",
    "# axs[0].fill_between(x,mean_cr_loss+sem_cr_loss,mean_cr_loss-sem_cr_loss,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_loss, label='Fixed sign')\n",
    "# axs[0].fill_between(x,mean_fs_loss+sem_fs_loss,mean_fs_loss-sem_fs_loss,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_posneg_loss, label='Fixed sign without neg output')\n",
    "# axs[0].fill_between(x,mean_fs_posneg_loss+sem_fs_posneg_loss,mean_fs_posneg_loss-sem_fs_posneg_loss,alpha=0.2)\n",
    "\n",
    "# axs[0].set_xlabel('Trianing epochs')\n",
    "# axs[0].set_ylabel('Training loss')\n",
    "# axs[0].legend()\n",
    "\n",
    "# # Plot accuracy\n",
    "# x = np.linspace(1,mean_cr_accuracy.shape[0],num=mean_cr_accuracy.shape[0],dtype='int32')\n",
    "# axs[0].plot(mean_cr_accuracy, label='Co-release')\n",
    "# axs[0].fill_between(x,mean_cr_accuracy+sem_cr_accuracy,mean_cr_accuracy-sem_cr_accuracy,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_accuracy, label='Fixed sign')\n",
    "# axs[0].fill_between(x,mean_fs_accuracy+sem_fs_accuracy,mean_fs_accuracy-sem_fs_accuracy,alpha=0.2)\n",
    "# axs[0].plot(mean_fs_posneg_accuracy, label='Fixed sign without neg output')\n",
    "# axs[0].fill_between(x,mean_fs_posneg_accuracy+sem_fs_posneg_accuracy,mean_fs_posneg_accuracy-sem_fs_posneg_accuracy,alpha=0.2)\n",
    "\n",
    "# axs[0].set_xlabel('Trianing epochs')\n",
    "# axs[0].set_ylabel('Training loss')\n",
    "# axs[0].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplhbmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
