{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ae8d40",
   "metadata": {},
   "source": [
    "# Behavioral simulation using EP-LHb-DAN model\n",
    "\n",
    "- Shun Li, Elliot Jerng\n",
    "- @2024/12/01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123454f6",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Enviornmen setup](#Environment-setup)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "3. [Model Training](#Model-Training)\n",
    "4. [Model analysis](#Model-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "56cdcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST MLP\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from torch.optim import Optimizer\n",
    "# import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfb80d",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fe947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "TOPK_INIT_METHODS = {\n",
    "    'xavier_normal': nn.init.xavier_normal_,\n",
    "    'xavier_uniform': nn.init.xavier_uniform_,\n",
    "    'kaiming_normal': nn.init.kaiming_normal_,\n",
    "    'kaiming_uniform': nn.init.kaiming_uniform_,\n",
    "    'orthogonal': nn.init.orthogonal_,\n",
    "    'normal': nn.init.normal_,\n",
    "    'uniform': nn.init.uniform_,\n",
    "    'eye': nn.init.eye_,\n",
    "}\n",
    "\n",
    "\n",
    "class TopKLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer that retains only the top K strongest synaptic weights for\n",
    "    in_features.\n",
    "\n",
    "    This module implements a linear transformation with weights constrained to \n",
    "    be positive. For each output neuron, only the top K weights are kept \n",
    "    , and the rest are set to zero. This simulates a neuron receiving \n",
    "    inputs only from its strongest synaptic connections.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    in_features : int\n",
    "        The number of input features.\n",
    "    \n",
    "    out_features : int\n",
    "        The number of output features.\n",
    "\n",
    "    K : int\n",
    "        The number of strongest synapses to keep per dendritic branch.\n",
    "\n",
    "    param_space : str, optional\n",
    "        The parameter space for the weights. Options are 'log' and 'presigmoid'.\n",
    "        If `'log'`, the weights are parameterized as exponentials of `pre_w`.\n",
    "        If `'presigmoid'`, the weights are parameterized as sigmoids of `pre_w`.\n",
    "        Defaults to 'log'.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    pre_w : torch.nn.Parameter\n",
    "        The raw weights before applying the exponential or sigmoid \n",
    "        transformation. Initialized with small negative values to ensure \n",
    "        positive weights after transformation.\n",
    "\n",
    "    K : int\n",
    "        The percentage of strongest synapses to keep per dendritic branch.\n",
    "\n",
    "    param_space : str\n",
    "        The parameter space for the weights.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "    forward(x)\n",
    "        Performs a forward pass through the layer.\n",
    "\n",
    "    weight()\n",
    "        Returns the transformed synaptic weights after applying the exponential \n",
    "        or sigmoid.\n",
    "    \n",
    "    weight_mask()\n",
    "        Returns a mask tensor indicating the top K synaptic connections per \n",
    "        output neuron.\n",
    "\n",
    "    pruned_weight()\n",
    "        Returns the pruned synaptic weights after applying the mask.\n",
    "\n",
    "    weighted_synapses(cell_weights, prune=False)\n",
    "        Returns the weighted synapses for a given set of cell\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - All weights are constrained to be positive.\n",
    "    - The prunning is done dynamically during the forward pass.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> import torch\n",
    "    >>> from torch import nn\n",
    "    >>> topk_linear = TopKLinear(in_features=10, out_features=5, K=3)\n",
    "    >>> x = torch.randn(4, 10)  # Batch of 4 samples\n",
    "    >>> output = topk_linear(x)\n",
    "    >>> print(output.shape)\n",
    "    torch.Size([4, 5])\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        K,\n",
    "        param_space='log',\n",
    "        init_method='xavier_normal',\n",
    "        sign_pattern='mixed',  # Default: mixed signs\n",
    "        update_sign=False,  # Default: sign matrix is not trainable\n",
    "    ):\n",
    "        super(TopKLinear, self).__init__()\n",
    "        self.pre_w = nn.Parameter(\n",
    "            torch.empty((out_features, in_features)), requires_grad=True)\n",
    "\n",
    "        self.sign_matrix = nn.Parameter(\n",
    "            torch.empty((out_features, in_features)), requires_grad=False)\n",
    "        self.sign_matrix.is_sign_matrix = True  # Set automatically during initialization\n",
    "        \n",
    "        # self.log_bias = nn.Parameter(\n",
    "        #     torch.ones((out_features)), requires_grad=True)\n",
    "        \n",
    "        self.K = round(K/100 * in_features)\n",
    "        self.param_space = param_space\n",
    "        self.init_method = init_method\n",
    "        self.update_sign = update_sign # update sign based on gradient\n",
    "\n",
    "        self.initialize_signs(sign_pattern)\n",
    "        self.initialize_weights()\n",
    "        self.final_weight = self.weight().detach()\n",
    "        # self.final_bias = self.bias().detach()\n",
    "\n",
    "    def initialize_signs(self, sign_pattern):\n",
    "        \"\"\"Initialize the sign matrix based on the desired pattern.\"\"\"\n",
    "        if sign_pattern == 'mixed':\n",
    "            self.sign_matrix.data.uniform_(-1, 1)\n",
    "            self.sign_matrix.data = self.sign_matrix.sign()  # Values: -1 or 1\n",
    "        elif sign_pattern == 'negative':\n",
    "            self.sign_matrix.data.fill_(-1)  # All weights negative\n",
    "        elif sign_pattern == 'positive':\n",
    "            self.sign_matrix.data.fill_(1)  # All weights positive\n",
    "        else:\n",
    "            raise ValueError(\"Invalid sign pattern. Choose 'mixed', 'negative' or 'positive'.\")\n",
    "\n",
    "    def initialize_weights(self, **kwargs):\n",
    "        \"\"\"Initialize the raw weights using the specified method.\"\"\"\n",
    "        # if method == \"normal\":\n",
    "        #     log_normal_init(tensor, kwargs.get(\"mean\", -1.0), kwargs.get(\"std\", 0.5))\n",
    "        # elif method == \"uniform\":\n",
    "        #     log_uniform_init(tensor, kwargs.get(\"a\", -2.0), kwargs.get(\"b\", 0.0))\n",
    "        # elif method == \"xavier\":\n",
    "        #     scaled_log_xavier_init(tensor, kwargs.get(\"scaling_factor\", 0.1))\n",
    "        # elif method == \"geometric\":\n",
    "        #     geometric_mean_init(tensor, kwargs.get(\"geometric_mean\", 1.0), kwargs.get(\"std\", 0.5))\n",
    "        \n",
    "        if self.init_method in TOPK_INIT_METHODS:\n",
    "            init_func = TOPK_INIT_METHODS[self.init_method]\n",
    "            init_func(self.pre_w, kwargs.get(\"scaling_factor\",2))\n",
    "            #TODO: Accept hyperparameters for initialization\n",
    "            # init_func(self.pre_w, **self.init_params) \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid initialization method: {self.init_method}. \"\n",
    "                f\"Choose from {list(TOPK_INIT_METHODS.keys())}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform the forward pass.\"\"\"\n",
    "        final_weight = self.weight()    \n",
    "        # final_bias = self.bias()\n",
    "\n",
    "        final_activation = torch.mm(x, final_weight.t())\n",
    "        # self.final_activation = final_activation.detach()\n",
    "        return final_activation\n",
    "    \n",
    "    def weight(self):\n",
    "        \"\"\"Return the final weights.\"\"\"\n",
    "        if self.param_space == 'log':\n",
    "            # self.pre_w.clamp(min=-10,max=10)\n",
    "            final_weight = self.pre_w.exp() * self.sign_matrix\n",
    "        elif self.param_space == 'presigmoid':\n",
    "            final_weight = torch.sigmoid(self.pre_w) * self.sign_matrix\n",
    "\n",
    "        # scale weight\n",
    "        #final_weight = final_weight * 0.001\n",
    "        self.final_weight = final_weight.detach()\n",
    "        return final_weight\n",
    "    \n",
    "    def bias(self):\n",
    "        \"\"\"Return the final bias.\"\"\"\n",
    "        if self.param_space == 'log':\n",
    "            # self.log_bias.clamp(min=-10,max=10)\n",
    "            final_bias = self.log_bias.exp()\n",
    "        elif self.param_space == 'presigmoid':\n",
    "            final_bias = torch.sigmoid(self.log_bias)\n",
    "\n",
    "        # scale weight\n",
    "        self.final_bias = final_bias.detach()\n",
    "        return final_bias\n",
    "    \n",
    "    def update_signs(self):\n",
    "        \"\"\"\n",
    "        Dynamically switch signs based on gradients\n",
    "        \"\"\"\n",
    "        if self.update_sign:\n",
    "            with torch.no_grad():\n",
    "                # Access gradients of log_weights\n",
    "                grads = self.pre_w.grad  \n",
    "\n",
    "                 # Compute the condition: W * grad(W) < 0\n",
    "                 # i.e. gradient opposes weight direction\n",
    "                flip_mask = (self.weight() * grads) < 0\n",
    "\n",
    "                # Flip the signs to match the gradient\n",
    "                self.sign_matrix[flip_mask] *= -1\n",
    "    \n",
    "    def decay_weights(self, weight_decay = 0.1):\n",
    "        with torch.no_grad():\n",
    "            self.pre_w.data -= (weight_decay * self.weight_mask())\n",
    "    \n",
    "    def weight_mask(self):\n",
    "        \"\"\"Generate a mask for the top K strongest connections.\"\"\"\n",
    "        topK_indices = torch.topk(self.pre_w, self.K, dim=-1, largest=True, sorted=False)[1]\n",
    "        # initialize and populate masking matrix\n",
    "        mask = torch.zeros_like(\n",
    "            self.pre_w, device=self.pre_w.device, dtype=self.pre_w.dtype)\n",
    "        mask[torch.arange(self.pre_w.shape[0])[:, None], topK_indices] = 1\n",
    "        return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corelease_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Model follows co-release: regular MLP\n",
    "    \n",
    "    if real: make DAN weights pure inhib\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=784, h1=512, h2=512, out_features=11, dropout_rate=0, real = False, \n",
    "                 combine_EI = False, dales_law = False, opto_neuron_percent = 0.4, batch_size = 256, \n",
    "                 opto_on = False, log_weights = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create layers\n",
    "        if log_weights:\n",
    "            \n",
    "            self.EP = TopKLinear(in_features=in_features, out_features=h1, K=100, sign_pattern=\"mixed\", update_sign=False)\n",
    "            self.bn1 = nn.BatchNorm1d(h1)\n",
    "            self.LHb = TopKLinear(in_features=h1, out_features=h2, K=100, sign_pattern=\"mixed\", update_sign=True)\n",
    "            self.bn2 = nn.BatchNorm1d(h2)\n",
    "            if dropout_rate != 0: self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.DAN = TopKLinear(in_features=h2, out_features=out_features, K=100, sign_pattern=\"negative\", update_sign=False)\n",
    "\n",
    "        else:\n",
    "            self.EP = nn.Linear(in_features, h1)\n",
    "            self.bn1 = nn.BatchNorm1d(h1)\n",
    "            self.LHb = nn.Linear(h1, h2)\n",
    "            self.bn2 = nn.BatchNorm1d(h2)\n",
    "            if dropout_rate != 0: self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.DAN = nn.Linear(h2, out_features)\n",
    "        \n",
    "        # store parameters\n",
    "        self.init_weights = self.record_params(calc_sign=False)\n",
    "        self.real = real\n",
    "        self.dales_law = dales_law\n",
    "        self.loss = []\n",
    "        self.opto_neuron_percent = opto_neuron_percent\n",
    "        self.opto_tag = False\n",
    "        self.opto_on = opto_on\n",
    "        self.log_weights = log_weights\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # store opto idx\n",
    "        self.opto_idx_batch = None\n",
    "        self.opto_idx_trainset = []\n",
    "        \n",
    "        # store activations\n",
    "        self.EP_activations_ls = []\n",
    "        # self.EP_activations = []\n",
    "        # self.LHb_activations = []\n",
    "        # self.DAN_activations = []\n",
    "\n",
    "        \n",
    "        # store gradients\n",
    "        self.gradient_history = defaultdict(list)\n",
    "        \n",
    "        # store distances/model output\n",
    "        # self.output = []\n",
    "        self.avg_distance = []\n",
    "        self.avg_distance_opto = []\n",
    "        self.avg_test_distance = []\n",
    "        self.avg_test_distance_opto = []\n",
    "        \n",
    "        # initialize DAN as purely inhibitory\n",
    "        if self.real and self.log_weights == False:\n",
    "            self.apply(self.real_circuit)\n",
    "            \n",
    "        # Real LHb-DAN (useful when initially excitatory neuron will always be excitatory)\n",
    "        # DAN_pos_neurons, DAN_neg_neurons = {}, {}\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for name, param in self.named_parameters():\n",
    "        #         if self.real == True and \"DAN.weight\" in name:\n",
    "        #             DAN_pos_neurons[name] = torch.sum(param.data, axis = 0) >= 0\n",
    "        #             DAN_neg_neurons[name] = -torch.sum(param.data, axis = 0) < 0\n",
    "\n",
    "        #             # make neuron all excitatory/inhibitory\n",
    "        #             param.data[:, DAN_pos_neurons[name]] = torch.sign(param[:, DAN_pos_neurons[name]]) * param[:, DAN_pos_neurons[name]]\n",
    "        #             param.data[:, DAN_neg_neurons[name]] = -torch.sign(param[:, DAN_neg_neurons[name]]) * param[:, DAN_neg_neurons[name]]\n",
    "        \n",
    "        # self.DAN_pos_neurons = DAN_pos_neurons\n",
    "        # self.DAN_neg_neurons = DAN_neg_neurons\n",
    "        \n",
    "    def real_circuit(self, m):\n",
    "        \"\"\"\n",
    "        initializes DAN as purely negative\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear): # check if m is a Linear layer\n",
    "            with torch.no_grad():        \n",
    "                if m is self.LHb:\n",
    "                    # Initialize LHb bias as positive\n",
    "                    m.bias.data = torch.abs(m.bias.data)\n",
    "\n",
    "                if m is self.DAN:\n",
    "                    # Initialize inhibitory weights and DAN weights with negative values\n",
    "                    m.weight.data = -torch.abs(m.weight.data)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        At Opto Activation:\n",
    "        x size = ([256, 512]) - batch size x layer size\n",
    "        \n",
    "        set certain % of EP hidden layer neurons' activation values = 95 percentile \n",
    "        of batch's activation values\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flatten image\n",
    "\n",
    "        # Pass through the first layer, apply batch normalization and activation\n",
    "        x = self.EP(x)  # Apply EP (TopKLinear or nn.Linear)\n",
    "        x = self.bn1(x)  # Apply BatchNorm1d\n",
    "        #x = torch.sigmoid(x)  # Apply sigmoid activation\n",
    "        if self.opto_tag:\n",
    "            self.EP_activations_ls.append(x)\n",
    "        \n",
    "        # Pass through the second layer, apply batch normalization and activation\n",
    "        x = self.LHb(x)  # Apply LHb (TopKLinear or nn.Linear)\n",
    "        x = self.bn2(x)  # Apply BatchNorm1d\n",
    "        #x = torch.sigmoid(x)  # Apply sigmoid activation\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        if self.dropout_rate!= 0: x = self.dropout(x)\n",
    "        \n",
    "        # Pass through the final layer\n",
    "        x = self.DAN(x)\n",
    "        \n",
    "        #x = torch.tanh(x)  # Apply tanh activation\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def record_params(self, calc_sign: bool=True):\n",
    "        # Save the network weights\n",
    "        recorded_params = {}\n",
    "        \n",
    "        # Loop through all modules\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear): # check if Linear Layer\n",
    "                # Store weights\n",
    "                if module.weight.requires_grad:\n",
    "                    with torch.no_grad():\n",
    "                        recorded_params[name + '.weight'] = module.weight.data.detach().cpu().clone()\n",
    "                else:\n",
    "                    recorded_params[name + '.weight'] = module.weight.data.detach().cpu().clone()\n",
    "\n",
    "                # Store bias\n",
    "                if module.bias is not None:\n",
    "                    if module.bias.requires_grad:\n",
    "                        with torch.no_grad():\n",
    "                            recorded_params[name + '.bias'] = module.bias.data.detach().cpu().clone()\n",
    "                    else:\n",
    "                        recorded_params[name + '.bias'] = module.bias.data.detach().cpu().clone()\n",
    "\n",
    "            elif isinstance(module, TopKLinear):  # Check if it's a TopKLinear layer\n",
    "                # Store pre_w\n",
    "                if module.pre_w.requires_grad:\n",
    "                    with torch.no_grad():\n",
    "                        recorded_params[name + '.pre_w'] = module.pre_w.data.detach().cpu().clone()\n",
    "                else:\n",
    "                    recorded_params[name + '.pre_w'] = module.pre_w.data.detach().cpu().clone()\n",
    "\n",
    "                # Store sign_matrix\n",
    "                if module.sign_matrix.requires_grad:\n",
    "                    with torch.no_grad():\n",
    "                        recorded_params[name + '.sign_matrix'] = module.sign_matrix.data.detach().cpu().clone()\n",
    "                else:\n",
    "                    recorded_params[name + '.sign_matrix'] = module.sign_matrix.data.detach().cpu().clone()\n",
    "\n",
    "                # Store final_weights\n",
    "                # Store pre_w\n",
    "                if module.final_weight.requires_grad:\n",
    "                    with torch.no_grad():\n",
    "                        recorded_params[name + '.final_weight'] = module.final_weight.data.detach().cpu().clone()\n",
    "                else:\n",
    "                    recorded_params[name + '.final_weight'] = module.final_weight.data.detach().cpu().clone()\n",
    "        # Calculate the percentage of positive, negative, and zero weights\n",
    "        if calc_sign:\n",
    "            for name, cur_data in recorded_params.items():\n",
    "                frac_pos = 100*(torch.sum(cur_data > 0)/cur_data.numel()).numpy()\n",
    "                frac_zero = 100*(torch.sum(cur_data == 0)/cur_data.numel()).numpy()\n",
    "                frac_neg = 100*(torch.sum(cur_data < 0)/cur_data.numel()).numpy()\n",
    "                print(name + f': Positive: {frac_pos:.2f}%; Negative: {frac_neg:.2f}%; Zero: {frac_zero:.2f}%')\n",
    "\n",
    "        return recorded_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "3c7079fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n",
    "        super(LogAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = param.grad.data\n",
    "                # print(is_sign_matrix)\n",
    "                if hasattr(param, \"is_sign_matrix\") and param.is_sign_matrix:\n",
    "                    # Apply specific update rule for sign_matrix in layer 2\n",
    "                    param.data.add_(-group['lr'] * grad.sign())\n",
    "                    param.data.clamp_(-1, 1)  # Keep sign_matrix values constrained if necessary\n",
    "                    print(\"Sign matrix updated\")\n",
    "                else:\n",
    "                    # Use default Adam optimizer for other parameters\n",
    "                    state = self.state[param]\n",
    "                    # State initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = 0\n",
    "                        state['exp_avg'] = torch.zeros_like(param.data)\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(param.data)\n",
    "                        if group['amsgrad']:\n",
    "                            state['max_exp_avg_sq'] = torch.zeros_like(param.data)\n",
    "\n",
    "                    exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                    beta1, beta2 = group['betas']\n",
    "\n",
    "                    state['step'] += 1\n",
    "\n",
    "                    # Decay the first and second moment running average coefficient\n",
    "                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                    if group['amsgrad']:\n",
    "                        max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                        torch.maximum(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                        denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    else:\n",
    "                        denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                    step_size = group['lr']\n",
    "                    param.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                    # for name, group in self.named_groups():\n",
    "                    #     print(name)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "1ffd93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWeightDecayOptimizer(Optimizer):\n",
    "    def __init__(self, params, optimizer, weight_decay=0.1):\n",
    "        # Store the parameters and inner optimizer\n",
    "        self.params = list(params)\n",
    "        self.optimizer = optimizer\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Set the optimizer's param_groups so the scheduler recognizes it\n",
    "        self.defaults = optimizer.defaults\n",
    "        self.param_groups = optimizer.param_groups\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        # Apply custom weight decay logic\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Apply weight decay to the given parameters\n",
    "        for param in self.params:\n",
    "            if param.requires_grad:\n",
    "                param.data -= lr * self.weight_decay * param.data\n",
    "        \n",
    "        # Step the wrapped optimizer\n",
    "        self.optimizer.step(closure)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "cb2db75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(outputs, labels, method = \"Mean\"):\n",
    "    \"\"\"\n",
    "    calculate distance between outputs and labels\n",
    "\n",
    "    method: Euclidean, Cosine, Mean\n",
    "    \"\"\"\n",
    "    if method == \"Euclidean\":\n",
    "        distance = torch.sqrt(torch.sum((outputs - labels) ** 2, dim=1)).mean()\n",
    "    elif method == \"Cosine\":\n",
    "        distance = F.cosine_similarity(outputs, labels, dim=1)\n",
    "    elif method == \"Mean\":\n",
    "        distance = torch.mean(torch.mean(outputs, dim=1) - torch.mean(labels, dim=1))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method\")\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "89e90b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(model, train_loader, test_loader, val_loader, epochs, params_ls, opto_category,\n",
    "                opto_on, loss_function, grad_clip_value = 4):\n",
    "    \"\"\"\n",
    "    train then test model after each epoch \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # store activation history \n",
    "    # EP_activation_history = []\n",
    "    # LHb_activation_history = []\n",
    "    # DAN_activation_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (data, (mod_target, target)) in enumerate(train_loader):\n",
    "            category = target.tolist()\n",
    "             \n",
    "            # get indices of opto_category in batch\n",
    "            model.opto_idx_batch = [z for z, x in enumerate(category) if x == opto_category]\n",
    "            model.opto_idx_trainset.append(model.opto_idx_batch)\n",
    "            model.opto_tag = True\n",
    "\n",
    "                    \n",
    "            # train\n",
    "            model.train()   \n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(data)\n",
    "            loss = criterion(output, mod_target)\n",
    "            training_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            clip_grad_value_(model.parameters(), \n",
    "                                     clip_value = grad_clip_value)\n",
    "\n",
    "            \n",
    "            # get highest activation neurons based on EP activations\n",
    "            model.opto_tag = False\n",
    "            if i % 40 == 0:\n",
    "                if opto_on:\n",
    "                    ep_current_act = model.EP_activations_ls\n",
    "                    ep_opto_idx = model.opto_idx_trainset\n",
    "                    top_act_neurons = choose_neuron(ep_current_act, ep_opto_idx, neuron_pct=0.05, largest = True)\n",
    "                    \n",
    "                    # Store the LHb gradient for the top EP activated neurons\n",
    "                    for neuron_idx in top_act_neurons:\n",
    "                        for name, param in model.named_parameters():\n",
    "                            if \"LHb.weight\" in name and param.grad is not None:\n",
    "                                # Append current gradient for this neuron\n",
    "                                # records the LHb input gradient of top neurons (single column)\n",
    "                                model.gradient_history[neuron_idx].append(param.grad[:, neuron_idx])\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update signs dynamically after gradient computation\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, TopKLinear):\n",
    "                    module.update_signs()  # Flip signs if needed\n",
    "            \n",
    "\n",
    "            # Validation Accuracy\n",
    "            if i % 50 == 0:\n",
    "                if val_loader is not None:\n",
    "                    model.eval()\n",
    "                    \n",
    "                    correct, total, correct_opto_category, total_opto_category = 0, 0, 0, 0\n",
    "                    \n",
    "                    for val_data, (val_mod_labels, val_labels) in val_loader:\n",
    "                        val_data, val_mod_labels, val_labels = val_data.to(device), val_mod_labels.to(device), val_labels.to(device)  \n",
    "                        val_outputs = model(val_data)\n",
    "        \n",
    "                        # if i % 50 == 0:\n",
    "                        #     model.output.append(torch.mean(val_outputs).item())                \n",
    "\n",
    "                        mask_opto_category = (val_labels == opto_category)\n",
    "                        \n",
    "                        # MSE: calculate distance\n",
    "                        if loss_function == \"MSE\":\n",
    "                            if i % 50 == 0:\n",
    "                                avg_distance = calculate_distance(val_outputs, val_mod_labels)\n",
    "                                model.avg_distance.append(avg_distance)\n",
    "                                \n",
    "                                avg_distance_opto = calculate_distance(val_outputs[mask_opto_category], val_mod_labels[mask_opto_category])\n",
    "                                model.avg_distance_opto.append(avg_distance_opto)\n",
    "\n",
    "                        # CrossEntropy: calculate accuracy\n",
    "                        elif loss_function == \"CrossEntropy\":\n",
    "                            _, predicted = torch.max(val_outputs.data, 1)\n",
    "                            total += val_labels.size(0)\n",
    "                            correct += (predicted == val_labels).sum()\n",
    "                            accuracy = 100 * correct / total\n",
    "                            val_accuracy.append(accuracy.cpu())\n",
    "                            \n",
    "                            correct_opto_category += (predicted[mask_opto_category] == val_labels[mask_opto_category]).sum().item()\n",
    "                            total_opto_category += mask_opto_category.sum().item()\n",
    "                            accuracy_opto_category = 100 * correct_opto_category / total_opto_category\n",
    "                            \n",
    "                            \n",
    "                    if loss_function == \"MSE\":\n",
    "                        print('Epoch [%d/%d], Iteration: %d, Loss: %.4f' %(epoch+1, epochs, i, loss.data))\n",
    "                        model.loss.append((loss.data, len(model.loss) * 10))\n",
    "\n",
    "                    elif loss_function == \"CrossEntropy\":\n",
    "                        print('Epoch [%d/%d], Iteration: %d, Loss: %.4f, Val Accuracy: %.4f' %(epoch+1, epochs, i, loss.data, accuracy))\n",
    "                        print(f'Validation Accuracy for label {opto_category}: {accuracy_opto_category:.2f}%')  \n",
    "\n",
    "\n",
    "        # lr scheduler step every epoch\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # Test model\n",
    "    model.eval()\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    correct_opto_category, total_opto_category = 0, 0\n",
    "    \n",
    "    for i, (test_data, (test_mod_labels, test_labels)) in enumerate(test_loader):\n",
    "        test_data, test_mod_labels, test_labels = test_data.to(device), test_mod_labels.to(device), test_labels.to(device)\n",
    "        test_outputs = model(test_data)\n",
    "        \n",
    "        mask_opto_category = (test_labels == opto_category)\n",
    "        \n",
    "        if loss_function == \"MSE\":\n",
    "            avg_test_distance = calculate_distance(test_outputs, test_mod_labels)\n",
    "            model.avg_test_distance.append(avg_test_distance)\n",
    "            avg_test_distance_opto = calculate_distance(test_outputs[mask_opto_category], test_mod_labels[mask_opto_category])\n",
    "            model.avg_test_distance_opto.append(avg_test_distance_opto)\n",
    "\n",
    "        elif loss_function == \"CrossEntropy\":\n",
    "            _, predicted = torch.max(test_outputs.data, 1)\n",
    "            \n",
    "            total += test_labels.size(0)\n",
    "            correct += (predicted == test_labels).sum()\n",
    "            accuracy = 100 * correct / total\n",
    "            test_accuracy.append(accuracy.cpu())\n",
    "\n",
    "            total_opto_category += mask_opto_category.sum().item()\n",
    "            correct_opto_category += (predicted[mask_opto_category] == test_labels[mask_opto_category]).sum().item()\n",
    "            accuracy_opto_category = 100 * correct_opto_category / total_opto_category\n",
    "            print(f'Test Accuracy for label {opto_category}: {accuracy_opto_category:.2f}%')\n",
    "\n",
    "    # Distance plot for MSE\n",
    "    if loss_function == \"MSE\":\n",
    "        distance_plot(avg_distance = model.avg_distance, avg_distance_opto = model.avg_distance_opto, \n",
    "                      avg_test_distance = model.avg_test_distance, avg_test_distance_opto = model.avg_test_distance_opto, \n",
    "                      loss_tup = model.loss)\n",
    "        model.avg_distance, model.avg_test_distance, model.avg_distance_opto, model.avg_test_distance_opto, model.loss = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "dbf1e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fMNIST(Dataset):\n",
    "    def __init__(self, original_data, opto_category, opto_target, random, out_features=10):\n",
    "        '''\n",
    "        random: if True, random set opto_category label to all zeros, else set to opto_target\n",
    "        '''\n",
    "        self.original_data = original_data\n",
    "        targets = torch.tensor(original_data.targets.tolist())\n",
    "        \n",
    "        # Default one-hot encoded targets\n",
    "        # one_hot_targets = F.one_hot(targets, num_classes=10).float()\n",
    "        \n",
    "        # Random targets: one-hot encoded targets scaled by target_dict values\n",
    "        target_dict = {0: -1.0, 1: 1.0, 2: -1.0, 3: 1.0, 4: 1.0, 5: 0, 6: -1.0, 7: 0, 8: 0, 9: 0}\n",
    "        random_targets = torch.stack([torch.full((out_features,), target_dict[t.item()]) for t in targets])\n",
    "        \n",
    "        # Opto targets\n",
    "        opto_targets = random_targets.clone()\n",
    "        opto_targets[targets == opto_category] = torch.ones(out_features) * opto_target\n",
    "        \n",
    "        # Convert targets to tuples: ([target_values], original_label)\n",
    "        # one_hot_targets = [(one_hot_targets[i], targets[i].item()) for i in range(len(targets))]\n",
    "        random_targets = [(random_targets[i], targets[i].item()) for i in range(len(targets))]\n",
    "        opto_targets = [(opto_targets[i], targets[i].item()) for i in range(len(targets))]\n",
    "        \n",
    "        # Set targets based on conditions\n",
    "        if random:\n",
    "            self.targets = random_targets\n",
    "        else:\n",
    "            self.targets = opto_targets\n",
    "        \n",
    "        # Normalize original data\n",
    "        self.data = torch.div(original_data.data.float(), 255)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.original_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "bcb40747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_changes(initial_params, trained_params):\n",
    "    \"\"\"\n",
    "    Plot initial vs. trained weights for all layers, dynamically handling varying parameter counts.\n",
    "    \"\"\"\n",
    "    print(trained_params.keys())\n",
    "    \n",
    "    # Calculate weight flips and changes\n",
    "    for key in initial_params:\n",
    "        n_weights = initial_params[key].numel()\n",
    "        n_flip = (initial_params[key].sign() * trained_params[key].sign() < 0).count_nonzero().item()\n",
    "        print(f\"{key} flipped: {100 * n_flip / n_weights:.2f}% ({n_flip}/{n_weights})\")\n",
    "\n",
    "        n_changed = (initial_params[key] != trained_params[key]).count_nonzero().item()\n",
    "        print(f\"{key} changed: {100 * n_changed / n_weights:.2f}% ({n_changed}/{n_weights})\")\n",
    "\n",
    "    # Plot initial vs trained values\n",
    "    num_params = len(initial_params)\n",
    "    fig, axs = plt.subplots(3, (num_params // 3) + (num_params % 3 > 0), figsize=(20, 7))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (key, ax) in enumerate(zip(initial_params.keys(), axs)):\n",
    "        ax.scatter(initial_params[key].numpy(), trained_params[key].numpy(), s=10, alpha=0.5)\n",
    "        ax.axhline(y=0, linewidth=2, color='r', ls='--')\n",
    "        ax.axvline(x=0, linewidth=2, color='r', ls='--')\n",
    "        ax.set_title(key)\n",
    "\n",
    "    # Hide extra subplots\n",
    "    for ax in axs[num_params:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "03b07bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_hist(initial_params, trained_params):    \n",
    "\n",
    "\n",
    "    # Plot flipped EP neuron weight changes\n",
    "    n_row = 1\n",
    "    n_col = 4\n",
    "\n",
    "    fig, axs = plt.subplots(n_row,n_col,figsize=(15, 10))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    key = list(initial_params)[0]\n",
    "    picked_neurons = torch.randint(1,input_size,(n_col,))\n",
    "\n",
    "    for i, neuron in enumerate(picked_neurons):\n",
    "        weight_history_difference = [trained_params[key][:,neuron].flatten()-\n",
    "                                    initial_params[key][:,neuron].flatten()]\n",
    "\n",
    "        axs.flatten()[i].hist(weight_history_difference)\n",
    "        axs.flatten()[i].set_xlabel('Training phase')\n",
    "        axs.flatten()[i].set_ylabel('Count')\n",
    "        axs.flatten()[i].set_title(f': %s (neuron %d)' %(key, neuron))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "7386f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_plot(avg_distance, avg_distance_opto, avg_test_distance, avg_test_distance_opto, loss_tup):\n",
    "    \"\"\"\n",
    "    Calculate distance from model output/prediction to true label.\n",
    "    Helpful to gauge MSE accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tensors to numpy arrays and flatten them if needed\n",
    "    values = [tensor.detach().cpu().numpy().flatten() for tensor in avg_distance]\n",
    "    test_values = [tensor.detach().cpu().numpy().flatten() for tensor in avg_test_distance]\n",
    "    \n",
    "    if avg_distance_opto is not None:\n",
    "        opto_values = [tensor.detach().cpu().numpy().flatten() for tensor in avg_distance_opto]\n",
    "        opto_test_values = [tensor.detach().cpu().numpy().flatten() for tensor in avg_test_distance_opto]\n",
    "        opto_values = np.concatenate(opto_values)\n",
    "        opto_test_values_mean = np.mean(np.concatenate(opto_test_values))\n",
    "\n",
    "    # Concatenate all arrays to create a single list of values\n",
    "    values = np.concatenate(values)\n",
    "    \n",
    "    test_values_mean = np.mean(np.concatenate(test_values))\n",
    "    \n",
    "\n",
    "    # Create subplots with 1 row and 2 columns\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    \n",
    "    # Training Loss Plot\n",
    "    loss, iterations = zip(*loss_tup)\n",
    "    ax1.plot(iterations, loss)\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Training Loss\")\n",
    "    \n",
    "    # Line plot on the left\n",
    "    ax2.plot(values, label='Overall')\n",
    "    if avg_distance_opto is not None:\n",
    "        ax2.plot(opto_values, label='Opto')\n",
    "    ax2.set_xlabel('Batches')\n",
    "    ax2.set_ylabel('Avg Training Distance')\n",
    "    ax2.set_title(\"Distance Plot\")\n",
    "    ax2.legend()\n",
    "\n",
    "    # Bar plot on the right\n",
    "    if avg_test_distance_opto is not None:\n",
    "        ax3.bar([\"Overall\", \"Opto\"], [test_values_mean, opto_test_values_mean], color = [\"blue\", \"orange\"])\n",
    "        ax3.set_ylabel(\"Test Distance\")\n",
    "        ax3.set_title(\"Test Distance Comparison\")\n",
    "    else:\n",
    "        ax3.bar([\"Overall\"], [test_values_mean], color = [\"blue\"])\n",
    "        ax3.set_ylabel(\"Test Distance\")\n",
    "        ax3.set_title(\"Test Distance Comparison\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "13a5ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_neuron(ep_current_act, ep_opto_idx, neuron_pct, largest):\n",
    "    \"\"\"\n",
    "    Return indices of the top activation values (?%) of neurons.\n",
    "\n",
    "    Args:\n",
    "    - ep_current_act: Tensor of activation values for each training batch.\n",
    "    - ep_opto_idx: List of indices within each batch where images are trained with opto.\n",
    "    - neuron_pct: Percentage of top neurons to consider (e.g., 0.05 for top 5%).\n",
    "\n",
    "    Returns:\n",
    "    - top_act_neurons: List of indices for the most commonly activated neurons.\n",
    "    \"\"\"\n",
    "    top_indices_ls = []\n",
    "\n",
    "    # Process each batch without nested loops\n",
    "    for batch_idx, opto_indices in enumerate(ep_opto_idx):\n",
    "        opto_activations = ep_current_act[batch_idx][opto_indices]  # Shape: (len(opto_indices), 512)\n",
    "        \n",
    "        # Calculate top neurons\n",
    "        top_k = int(neuron_pct * opto_activations.size(1))\n",
    "        \n",
    "        # Get top indices across all opto activations in batch\n",
    "        _, top_indices = torch.topk(opto_activations, top_k, dim=1, largest=largest)\n",
    "        \n",
    "        top_indices_ls.append(top_indices.flatten())\n",
    "    \n",
    "    # Concatenate all indices and count occurrences\n",
    "    top_indices_concat = torch.cat(top_indices_ls)\n",
    "    counter = Counter(top_indices_concat.tolist())\n",
    "    top_most_common = counter.most_common(int(neuron_pct * ep_current_act[0].size(1)))\n",
    "    \n",
    "    # Extract most common neuron indices\n",
    "    top_act_neurons = [item[0] for item in top_most_common]\n",
    "    \n",
    "    return top_act_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "e34b37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title=None):\n",
    "    \"\"\"\n",
    "    Shows a single-channel (grayscale) MNIST image without any normalization.\n",
    "    \"\"\"\n",
    "    npimg = img.numpy()\n",
    "\n",
    "    plt.imshow(npimg,cmap = \"gray\" ,vmin=0, vmax=1)  # Ensuring the pixel values are between 0 and 1\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "dbff03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_opto_loader(dataloader, opto_category, plot=False):\n",
    "    # get only opto images\n",
    "    # Placeholder for filtered images and labels\n",
    "    filtered_images = []\n",
    "    filtered_labels = []\n",
    "\n",
    "    # Iterate over the random_train_loader\n",
    "    for img, label in dataloader:\n",
    "        # Old labels are at index 1\n",
    "        old_labels = label[1]\n",
    "        new_labels = label[0]\n",
    "        \n",
    "        # Find indices where the old label corresponds to 8\n",
    "        mask = (old_labels == opto_category)\n",
    "        \n",
    "        # If there are any matches, append the corresponding images and labels\n",
    "        if mask.any():\n",
    "            filtered_images.append(img[mask])\n",
    "            filtered_labels.append(new_labels[mask])\n",
    "\n",
    "    # Concatenate filtered data\n",
    "    if filtered_images and filtered_labels:  # Ensure there's filtered data\n",
    "        # Concatenate lists into tensors\n",
    "        filtered_images = torch.cat(filtered_images, dim=0)\n",
    "        filtered_labels = torch.cat(filtered_labels, dim=0)\n",
    "\n",
    "        # Create a new DataLoader with the filtered data\n",
    "        opto_dataset = TensorDataset(filtered_images, filtered_labels)\n",
    "        opto_loader = DataLoader(opto_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "        # Verify by iterating over the filtered_loader\n",
    "        if plot:\n",
    "            for img, label in opto_loader:\n",
    "                imshow(img[1], title=f'Label: {label[1].item()}')\n",
    "                break  # Print the first batch for verification\n",
    "    else:\n",
    "        print(\"No samples with old label 8 found.\")\n",
    "\n",
    "    return opto_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "2e1a87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, input_loader, layer_types=(TopKLinear, torch.nn.Linear)):\n",
    "    model.eval()\n",
    "    activations = {}\n",
    "\n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach()\n",
    "            \n",
    "        return hook\n",
    "\n",
    "    hooks = []\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, layer_types):\n",
    "            # Register a hook with a specific layer name\n",
    "            hooks.append(layer.register_forward_hook(hook_fn(name)))\n",
    "    \n",
    "    # Perform a forward pass\n",
    "    for img, label in input_loader:\n",
    "        model(img)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "87181de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_activation(activations, \n",
    "                          plot = [True, True, True],\n",
    "                          layers = [\"EP\", \"LHb\", \"DAN\"],\n",
    "                          labels = [\"random\", \"reward\", \"punish\"],\n",
    "                          colors = [\"lightblue\", \"blue\", \"red\"]):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize = (20, 7), sharex=False, sharey=False)\n",
    "    activations = [activations[i] for i in range(len(activations)) if plot[i]]\n",
    "    labels = [labels[i] for i in range(len(activations)) if plot[i]]\n",
    "    colors = [colors[i] for i in range(len(activations)) if plot[i]]\n",
    "\n",
    "    # Plot distributions of activation values\n",
    "    for i, layer in enumerate(layers):\n",
    "        for d, activation in enumerate(activations):\n",
    "            act = activation[layer].flatten()\n",
    "            axs[i].hist(act, label = labels[d], color = colors[d], alpha = 0.7)\n",
    "            \n",
    "        axs[i].set_title(\"Activation Change in \" + layer)\n",
    "        axs[i].legend()\n",
    "        axs[i].set_xlabel(\"Activation Value\")\n",
    "\n",
    "    # plt.savefig(f\"{train_type}_activations.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "3a71af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterboxplot(data, labels, ax, vert=True, colors = [\"lightblue\", \"blue\", \"red\"], jitter = 0.02):\n",
    "    \"\"\"\n",
    "    Scatter box plot for EI index\n",
    "    \"\"\"\n",
    "    ax.boxplot(data, labels = labels, vert = vert)\n",
    "    \n",
    "    for i, d in enumerate(data):\n",
    "        category = np.ones(len(d)) * (i + 1) + np.random.normal(0, jitter, len(d))\n",
    "        if vert:\n",
    "            ax.scatter(category, d, alpha = 0.6, color = colors[i], label = labels[i])\n",
    "        else:\n",
    "            ax.scatter(d, category, alpha = 0.6, color = colors[i], label = labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98decb",
   "metadata": {},
   "source": [
    "## Data perparation\n",
    "\n",
    "- Load Data into Pre-Train (CrossEntropy) and Post-Train (MSE) Sets\n",
    "- Currenly pre-train data is not used and will destroy learning\n",
    "- Post-train data contains random, reward, and punish dataset\n",
    "    - Random: each DAN neuron should output all 0 / -1 / 1, `opto_category` should output all 0s\n",
    "    - Reward: `opto_category` should output all 1, others remain unchanged\n",
    "    - Punish: `opto_category` should output all -1, others remain unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "06f41f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET OPTO CATEGORY\n",
    "opto_category = 8\n",
    "out_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "4e83744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_data = datasets.FashionMNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "# pre training scalar set\n",
    "# pre_train_fMNIST = fMNIST(original_data = train_data, opto_category = opto_category,\n",
    "#                          loss_function = \"MSE\", opto_on = False, task = 1, random = False )\n",
    "\n",
    "# random training scalar set\n",
    "random_train_fMNIST = fMNIST(original_data = train_data, opto_category = opto_category, \n",
    "                             opto_target = 0, random = True, out_features = out_features)\n",
    "\n",
    "# post train non-scalar set\n",
    "reward_train_fMNIST = fMNIST(original_data = train_data, opto_category = opto_category,\n",
    "                            opto_target = 1, random = False, out_features = out_features)\n",
    "\n",
    "# post train non-scalar set\n",
    "punish_train_fMNIST = fMNIST(original_data = train_data, opto_category = opto_category,\n",
    "                            opto_target = -1, random = False, out_features = out_features)\n",
    "\n",
    "# test set\n",
    "original_test_data = datasets.FashionMNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())\n",
    "\n",
    "# pre_test_data = fMNIST(original_data = original_test_data, opto_category = opto_category, \n",
    "#                   loss_function = \"MSE\", opto_on = False, task = 1, random = False)\n",
    "\n",
    "random_test_data = fMNIST(original_data = original_test_data, opto_category = opto_category, \n",
    "                        opto_target = 0, random = True, out_features = out_features)\n",
    "\n",
    "\n",
    "reward_test_data = fMNIST(original_data = original_test_data, opto_category = opto_category, \n",
    "                        opto_target = 1, random = False, out_features = out_features)\n",
    "\n",
    "punish_test_data = fMNIST(original_data = original_test_data, opto_category = opto_category, \n",
    "                        opto_target = -1, random = False, out_features = out_features)\n",
    "\n",
    "# Split train data into train + validation set\n",
    "train_size = 50000\n",
    "val_size = 10000\n",
    "batch_size = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# pre_train_data, pre_val_data = random_split(pre_train_fMNIST, [train_size, val_size], \n",
    "#                                                           generator=torch.Generator().manual_seed(42))\n",
    "# # DataLoaders\n",
    "# pre_train_loader = DataLoader(dataset=pre_train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "# pre_val_loader = DataLoader(dataset=pre_val_data, batch_size=batch_size, shuffle=False, generator=torch.Generator(device=device))\n",
    "# pre_test_loader = DataLoader(dataset=pre_test_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "\n",
    "\n",
    "# Split random data into train + validation set\n",
    "random_train_data, random_val_data = random_split(random_train_fMNIST, [train_size, val_size], \n",
    "                                                          generator=torch.Generator().manual_seed(42))\n",
    "random_train_loader = DataLoader(dataset=random_train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "random_val_loader = DataLoader(dataset=random_val_data, batch_size=batch_size, shuffle=False, generator=torch.Generator(device=device))\n",
    "random_test_loader = DataLoader(dataset=random_test_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "\n",
    "\n",
    "\n",
    "# Split reward data into train + validation set\n",
    "reward_train_data, reward_val_data = random_split(reward_train_fMNIST, [train_size, val_size], \n",
    "                                                          generator=torch.Generator().manual_seed(42))\n",
    "reward_train_loader = DataLoader(dataset=reward_train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "reward_val_loader = DataLoader(dataset=reward_val_data, batch_size=batch_size, shuffle=False, generator=torch.Generator(device=device))\n",
    "reward_test_loader = DataLoader(dataset=reward_test_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "\n",
    "\n",
    "# Split punish data into train + validation set\n",
    "punish_train_data, punish_val_data = random_split(punish_train_fMNIST, [train_size, val_size], \n",
    "                                                          generator=torch.Generator().manual_seed(42))\n",
    "punish_train_loader = DataLoader(dataset=punish_train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "punish_val_loader = DataLoader(dataset=punish_val_data, batch_size=batch_size, shuffle=False, generator=torch.Generator(device=device))\n",
    "punish_test_loader = DataLoader(dataset=punish_test_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "\n",
    "# for weight changes plot\n",
    "image, label = train_data.__getitem__(0)\n",
    "input_size = image.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "41b0ffb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # check pre-train\n",
    "# count = 0\n",
    "# for img, label in pre_train_loader:\n",
    "#     while count < 1:\n",
    "#         # Set the print options to display the full tensor\n",
    "#         torch.set_printoptions(threshold=10000)\n",
    "#         print(label)\n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ef970f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# check random-train\n",
    "count = 0\n",
    "for img, label in random_train_loader:\n",
    "    while count < 1:\n",
    "        # Set the print options to display the full tensor\n",
    "        torch.set_printoptions(threshold=10000) \n",
    "        # Print label[0, idx] where idx corresponds to idx where label[1] is the opto category\n",
    "        print(label[0][(label[1] == opto_category).numpy()])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "7b633375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# check post-train (MSE loss)\n",
    "count = 0\n",
    "for img, label in reward_train_loader:\n",
    "    while count < 1:\n",
    "        # Set the print options to display the full tensor\n",
    "        torch.set_printoptions(threshold=10000)\n",
    "        print(label[0][(label[1] == opto_category).numpy()])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "cea10a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.]])\n"
     ]
    }
   ],
   "source": [
    "# check post-train (MSE loss)\n",
    "count = 0\n",
    "for img, label in punish_train_loader:\n",
    "    while count < 1:\n",
    "        # Set the print options to display the full tensor\n",
    "        torch.set_printoptions(threshold=10000)\n",
    "        print(label[0][(label[1] == opto_category).numpy()])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "fdeb0458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmmElEQVR4nO3de3SUV73/8c8QkiFAMpBCbtyalLb0yEWlJWIpUMkiRO0CWrX1qKUupas0aFvsRVy2UI8aqVJ76sHWK1h70bar0GNl4amUgOfIRSiIaEES0xIkl4LNTBJyg+zfH/wYnZIAezOTnYT3a61nLTLzfPN8s+eZfHgyk28CxhgjAAC6WT/fDQAALk4EEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEENCJN998U4FAQN/5znfi9jnLysoUCARUVlYWt88J9GYEEPqMNWvWKBAIaOfOnb5b6VY7duzQnXfeqcmTJys5OVmBQMD6c/z+97/XtGnTNHDgQGVnZ+uLX/yiGhsbE9At8E8EENDLrV+/Xj/+8Y8VCASUn59vXb9nzx7NmjVLx48f16OPPqrPf/7z+uEPf6iPf/zjCegW+Kf+vhsAcGEWLVqkBx54QKmpqVq8eLH++te/WtV/5Stf0dChQ1VWVqb09HRJ0qWXXqqFCxfqf/7nfzR79uxEtA1wBYSLS1tbmx566CFNnjxZoVBIgwYN0nXXXadNmzZ1WfPd735XY8aMUWpqqmbMmKF9+/adsc/+/fv1sY99TBkZGRowYICuvvpq/fd///c5+zl+/Lj279+vo0ePOn9NWVlZSk1NdaqNRCJ69dVX9elPfzoaPpJ06623avDgwXr++eed+wLOhQDCRSUSiejHP/6xZs6cqRUrVmj58uV6++23VVRUpD179pyx/1NPPaXHH39cJSUlWrp0qfbt26cPfehDqq2tje7z5z//WR/4wAf0xhtv6Mtf/rJWrlypQYMGad68eVq7du1Z+9mxY4euuuoq/dd//Ve8v9Tz8qc//UknTpzQ1VdfHXN7SkqK3vve92r37t1e+sLFgR/B4aIydOhQvfnmm0pJSYnetnDhQo0bN07f+9739JOf/CRm//Lych08eFAjRoyQJM2ZM0cFBQVasWKFHn30UUnSXXfdpdGjR+sPf/iDgsGgJOnOO+/UtGnT9MADD2j+/Pnd9NXZq66uliTl5OSccV9OTo5+97vfdXdLuIhwBYSLSlJSUjR8Ojo69I9//CN6BfD666+fsf+8efOi4SNJU6ZMUUFBgdavXy9J+sc//qHXXntNn/jEJ9TQ0KCjR4/q6NGjOnbsmIqKinTw4EH9/e9/77KfmTNnyhij5cuXx/cLPU/Nzc2SFA3OfzVgwIDo/UAicAWEi87PfvYzrVy5Uvv371d7e3v09ry8vDP2vfzyy8+47Yorroi+NlJeXi5jjB588EE9+OCDnR6vrq4uJsRcNDY2xrwtOikpScOHD7+gzykp+tpRa2vrGfe1tLQ4v7YEnA8CCBeVp59+WrfddpvmzZun++67T5mZmUpKSlJpaakqKiqsP19HR4ck6d5771VRUVGn+4wdO/aCepak73znO3r44YejH48ZM0ZvvvnmBX/e0z96O/2juH9VXV2t3NzcCz4G0BUCCBeVF198Ufn5+XrppZdifmFz2bJlne5/8ODBM27761//qksvvVSSor93k5ycrMLCwvg3/P/deuutmjZtWvTjeF2ZjB8/Xv3799fOnTv1iU98Inp7W1ub9uzZE3MbEG+8BoSLSlJSkiTJGBO9bfv27dq6dWun+69bty7mNZwdO3Zo+/btKi4uliRlZmZq5syZ+sEPftDpVcTbb7991n7O923Y+fn5KiwsjG7XXnvtWffvyv79+3Xo0KHox6FQSIWFhXr66afV0NAQvf3nP/+5Ghsb+WVUJBRXQOhzfvrTn2rDhg1n3H7XXXfpox/9qF566SXNnz9fH/nIR1RZWaknn3xS//Zv/9bp6JmxY8dq2rRpWrRokVpbW/XYY4/pkksu0f333x/dZ9WqVZo2bZomTJighQsXKj8/X7W1tdq6dasOHz6sP/7xj132umPHDl1//fVatmyZ8xsR3nrrLf385z+XpOgYoq9//euSTv2o7jOf+Ux036uuukozZsyImUf3jW98Qx/84Ac1Y8YM3X777Tp8+LBWrlyp2bNna86cOU49AefFAH3E6tWrjaQut6qqKtPR0WG++c1vmjFjxphgMGje9773mVdeecUsWLDAjBkzJvq5KisrjSTz7W9/26xcudKMGjXKBINBc91115k//vGPZxy7oqLC3HrrrSY7O9skJyebESNGmI9+9KPmxRdfjO6zadMmI8ls2rTpjNuWLVvm/HWf/hydbTNmzIjZt7PbjDHmd7/7nfngBz9oBgwYYIYPH25KSkpMJBJx7gk4HwFj/uVnEQAAdBNeAwIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwIse94uoHR0dOnLkiNLS0pz+tj0AwC9jjBoaGpSbm6t+/bq+zulxAXTkyBGNGjXKdxsAgAtUVVWlkSNHdnl/j/sRXFpamu8WAABxcK7v5wkLoFWrVunSSy/VgAEDVFBQoB07dpxXHT92A4C+4VzfzxMSQL/85S+1ZMkSLVu2TK+//romTZqkoqIi1dXVJeJwAIDeKBED5qZMmWJKSkqiH588edLk5uaa0tLSc9aGw+GzDpRkY2NjY+sdWzgcPuv3+7hfAbW1tWnXrl0xf5yrX79+Kiws7PRvrrS2tioSicRsAIC+L+4BdPToUZ08eVJZWVkxt2dlZammpuaM/UtLSxUKhaIb74ADgIuD93fBLV26VOFwOLpVVVX5bgkA0A3i/ntAw4YNU1JSkmpra2Nur62tVXZ29hn7B4NBBYPBeLcBAOjh4n4FlJKSosmTJ2vjxo3R2zo6OrRx40ZNnTo13ocDAPRSCZmEsGTJEi1YsEBXX321pkyZoscee0xNTU367Gc/m4jDAQB6oYQE0M0336y3335bDz30kGpqavTe975XGzZsOOONCQCAi1fAGGN8N/GvIpGIQqGQ7zYAABcoHA4rPT29y/u9vwsOAHBxIoAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF70991AvKSmplrXLFmyxOlY3/jGN5zq0DclJSVZ1xhjrGsCgYB1jat+/ez/b+rS38mTJ61rXHTXcVwtWrTIuuaNN95wOlZZWZlTXSJwBQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXgSMy1TEBIpEIgqFQtZ1d911l3XNY489Zl0jSZdffrl1TXl5udOx+poRI0ZY1/z9739PQCdAz3Hw4EHrmqNHjzoda+rUqU51LsLhsNLT07u8nysgAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCiv+8G4mXo0KHWNa7D/LKysqxr5s6da12zYsUK65o//OEP1jWu82gHDx5sXePyOO3du9e6prGx0bpGktavX29d47Lmx44ds64JBoPWNR0dHdY13cmlv379uu//zZFIxLrG5Rw/cuSIdU1zc7N1TU/DFRAAwAsCCADgRdwDaPny5QoEAjHbuHHj4n0YAEAvl5DXgN7znvfot7/97T8P0r/PvNQEAIiThCRD//79lZ2dnYhPDQDoIxLyGtDBgweVm5ur/Px8fepTn9KhQ4e63Le1tVWRSCRmAwD0fXEPoIKCAq1Zs0YbNmzQE088ocrKSl133XVqaGjodP/S0lKFQqHoNmrUqHi3BADogeIeQMXFxfr4xz+uiRMnqqioSOvXr1d9fb2ef/75TvdfunSpwuFwdKuqqop3SwCAHijh7w4YMmSIrrjiCpWXl3d6fzAYdPoFOwBA75bw3wNqbGxURUWFcnJyEn0oAEAvEvcAuvfee7V582a9+eab+v3vf6/58+crKSlJn/zkJ+N9KABALxb3H8EdPnxYn/zkJ3Xs2DENHz5c06ZN07Zt2zR8+PB4HwoA0IsFjOskygSJRCIKhUJKTU1VIBA477qVK1daH8t1UOPAgQOta8aOHWtdU1dXZ11z/Phx6xpXLuvncrolJSVZ16SkpFjXuB5r9OjR3XIclyGcrm/qsXnunebyWq7LcVzWzvUlAJfz9e2337aueeONN6xrUlNTrWsk6bnnnrOu2bBhg9OxwuGw0tPTu7yfWXAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4EXC/yCdq5ycHKvhi9OmTbM+xt/+9jfrGklOk72PHDliXXPixAnrGpcBoc3NzdY1ktTe3m5d4zLI1WX4ZH19vXWN5DZQc/fu3dY1LoMkXdauf3+3p/iAAQOsa1weJ5dhpC5cnn+SW38tLS3WNS7fU/Ly8qxrJOmtt96yrnEdRnouXAEBALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADAix47DTscDltNw45EItbHcJ0UXFdXZ13zzjvvWNe0tbVZ19is2WnJycnWNZLb5GiXCd+tra3WNa6PrcuxXCaQnzx50rrGhctkZsltsrXLeeRyjrusXXdN3Zbczj2X9W5sbLSukaTq6mqnukTgCggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvOixw0hTU1OtBmump6dbH+Nvf/ubdY3kNozUGGNd4zLs02UwpkuN1H1DF11qXIaySm5fU2pqqnWNy1BWl4G2rkNPXdbPZbCoy/BXl+O0t7db10hu6+dyDrkMFs3IyLCukaTRo0c71SUCV0AAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4EWPHUba2tpqNRDRZZif68DKkSNHWtcEAgHrGpcBii4DTF2GaUrSgAEDrGtSUlKsa1zWwXUIp8vQWJdhri7nw7hx46xrXB9bl4GaLo+TyzDS5uZm6xrXYaQu31fq6+uta1wGi7o+trt373aqSwSugAAAXhBAAAAvrANoy5YtuuGGG5Sbm6tAIKB169bF3G+M0UMPPaScnBylpqaqsLBQBw8ejFe/AIA+wjqAmpqaNGnSJK1atarT+x955BE9/vjjevLJJ7V9+3YNGjRIRUVFamlpueBmAQB9h/UrjcXFxSouLu70PmOMHnvsMX31q1/V3LlzJUlPPfWUsrKytG7dOt1yyy0X1i0AoM+I62tAlZWVqqmpUWFhYfS2UCikgoICbd26tdOa1tZWRSKRmA0A0PfFNYBqamokSVlZWTG3Z2VlRe97t9LSUoVCoeg2atSoeLYEAOihvL8LbunSpQqHw9GtqqrKd0sAgG4Q1wDKzs6WJNXW1sbcXltbG73v3YLBoNLT02M2AEDfF9cAysvLU3Z2tjZu3Bi9LRKJaPv27Zo6dWo8DwUA6OWs3wXX2Nio8vLy6MeVlZXas2ePMjIyNHr0aN199936+te/rssvv1x5eXl68MEHlZubq3nz5sWzbwBAL2cdQDt37tT1118f/XjJkiWSpAULFmjNmjW6//771dTUpNtvv1319fWaNm2aNmzY4DQ3DADQdwWMy/TFBIpEIgqFQho5cqTVsNBf//rX1seqqKiwrnHV1tZmXeMyLNVlCKfLYExJSkpKsq5xOd1c1s6Vy5onJydb17gMx3QZ5NqdT2+X/2S6DHJ1+ZpczlXJ7XFKS0uzrnE57979buPz9fzzz1vXLF++3OlY4XD4rK/re38XHADg4kQAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAX1n+OobtEIhGrKc2RSMT6GO+88451TXfq3797Hh6XKcuS2+TtYDBoXeMyrdt17Vz6c5mGffToUesal4nJLtOmXbW0tFjXuEypdvmaTpw4YV0juU34dnmcGhoarGuGDh1qXSO5neOJwhUQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHjRY4eRtrW1WQ2hbGtrsz6G66BGl0GXLjUugxpdBne6Did0OZaL7hrKKklNTU3WNS7DXF0GubqcDy41rnUuQ1lduKydK5fH1mUYqctz0PWxPXbsmFNdInAFBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABe9NhhpC0tLVb7t7e3J6iTM9n2JnXvsEFbrsMdXepaW1utawYNGmRd051cHieXgZXGGOsaVy7HcnleuAwwdRki7Lp2LnUuz4vufGy7c5jruXAFBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABe9NhhpLZOnDhhXTN48OAEdBI/LkMuXYY7ugzGlNwGiw4ZMsS6ZsCAAdY1zc3N1jWSFAgErGtc+mtsbLSuSUlJsa5x5bIOLkMuXY7Tv7/9ty3XYZ8uzw2X86Gtrc26xuV7ntR9Q47PB1dAAAAvCCAAgBfWAbRlyxbdcMMNys3NVSAQ0Lp162Luv+222xQIBGK2OXPmxKtfAEAfYR1ATU1NmjRpklatWtXlPnPmzFF1dXV0e+655y6oSQBA32P9al5xcbGKi4vPuk8wGFR2drZzUwCAvi8hrwGVlZUpMzNTV155pRYtWqRjx451uW9ra6sikUjMBgDo++IeQHPmzNFTTz2ljRs3asWKFdq8ebOKi4u7fItmaWmpQqFQdBs1alS8WwIA9EBx/z2gW265JfrvCRMmaOLEibrssstUVlamWbNmnbH/0qVLtWTJkujHkUiEEAKAi0DC34adn5+vYcOGqby8vNP7g8Gg0tPTYzYAQN+X8AA6fPiwjh07ppycnEQfCgDQi1j/CK6xsTHmaqayslJ79uxRRkaGMjIy9PDDD+umm25Sdna2KioqdP/992vs2LEqKiqKa+MAgN7NOoB27typ66+/Pvrx6ddvFixYoCeeeEJ79+7Vz372M9XX1ys3N1ezZ8/Wf/zHfygYDMavawBAr2cdQDNnzjzrYL/f/OY3F9SQK5ehgS6DECWpoaHBusZlSKjLUEOXoYsuQyRduQzUbGlpsa45fvy4dY3kdk501wBYl4GVrlwGVrqsncs6uJzjrs91l8GnLkNCXc5X1wGrLt9XEoVZcAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPAi7n+S2xeXicnt7e1Ox3KZQusygdblT1i4TO91nRTck6d1u0zdltzW3GUatstxXCZHu0y1dj2Wi4EDB1rX9PTp7eFw2LrGZdK5y9RtSQqFQk51icAVEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB40WeGkboMT+zo6HA6lssAxdTUVOsalyGXLl9Tdw5ldRkk6TIs1WUoq+Q2YNVlYKXLIEmX47iug0udy4BVl3PIZe1cn+utra3WNS7rkJ6ebl3jOpR12LBhTnWJwBUQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHjRY4eRDhw40GoI5dChQ62P0dzcbF0juQ1qdBmG2NbWZl3jMljUZeCi5DYA1mXAalJSknWNy+BOye2xdanprqGnLmvnWudyPrg8B12eF66DO5uamqxrXIaRujwvXAYcX0hdInAFBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABe9NhhpOnp6VbDDevq6qyP4TLUUHIbJOky1NBlsKjLUEOX4YmSNGjQIOsal0GINkNpT3MZECq5DeF0+Zq6a2ClK5fhne+88451jcsgXJd1cDmHJLfHyeVrcllvl+9DkvtzIxG4AgIAeEEAAQC8sAqg0tJSXXPNNUpLS1NmZqbmzZunAwcOxOzT0tKikpISXXLJJRo8eLBuuukm1dbWxrVpAEDvZxVAmzdvVklJibZt26ZXX31V7e3tmj17dszrG/fcc49+9atf6YUXXtDmzZt15MgR3XjjjXFvHADQu1m9GrVhw4aYj9esWaPMzEzt2rVL06dPVzgc1k9+8hM9++yz+tCHPiRJWr16ta666ipt27ZNH/jAB+LXOQCgV7ug14DC4bAkKSMjQ5K0a9cutbe3q7CwMLrPuHHjNHr0aG3durXTz9Ha2qpIJBKzAQD6PucA6ujo0N13361rr71W48ePlyTV1NQoJSVFQ4YMidk3KytLNTU1nX6e0tJShUKh6DZq1CjXlgAAvYhzAJWUlGjfvn36xS9+cUENLF26VOFwOLpVVVVd0OcDAPQOTr+RtHjxYr3yyivasmWLRo4cGb09OztbbW1tqq+vj7kKqq2tVXZ2dqefKxgMOv8iJACg97K6AjLGaPHixVq7dq1ee+015eXlxdw/efJkJScna+PGjdHbDhw4oEOHDmnq1Knx6RgA0CdYXQGVlJTo2Wef1csvv6y0tLTo6zqhUEipqakKhUL63Oc+pyVLligjI0Pp6en6whe+oKlTp/IOOABADKsAeuKJJyRJM2fOjLl99erVuu222yRJ3/3ud9WvXz/ddNNNam1tVVFRkb7//e/HpVkAQN8RMMYY3038q0gkolAopAkTJlgNhlyxYoX1sVwnNLgOAbSVkpJiXWMzwPU019fgumsopMt6uwwVldwGi4ZCIesal4GVLuvg+vR2eWxdzj2XIZwtLS3WNY2NjdY1ktvgTpfn7elfabHR0dFhXSPpjHcpn48f/ehHVvu3trbqP//zPxUOh5Went7lfsyCAwB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBc9dhp2//79rSYnv/jii9bHcp1q3dbW1i3HcpkUfMUVV1jXuEwxdq1zmbJ84sQJ6xrXP+3eXf25TDJ26c316e3Sn8v54DId3aXGdR1cjuXyOLlM0Hb5/iBJgwYNsq6pr6+32r+xsVHTp09nGjYAoGcigAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBf9fTfQFdsBj3PnzrU+hstQPslt+GR7e7t1jctAyJ4+qNHFhAkTrGueeeYZp2M1NjZa17icD4MHD7auSU1Nta7p39/tKT5w4EDrmlAo5HQsWy7DgF0Hd7o8tnV1ddY11dXV1jXvvPOOdY1r3fr16632P9/vd1wBAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXAeM6iTJBIpGIQqGQ+vXrZzXs0nXYIPqm4uJip7qWlhbrGpfBoklJSdY1LsNpXYbgSm791dfXW9e4DH+tqamxrgmHw9Y1ktTc3OxUh1PC4bDS09O7vJ8rIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwoscOIwUA9G4MIwUA9EgEEADAC6sAKi0t1TXXXKO0tDRlZmZq3rx5OnDgQMw+M2fOVCAQiNnuuOOOuDYNAOj9rAJo8+bNKikp0bZt2/Tqq6+qvb1ds2fPVlNTU8x+CxcuVHV1dXR75JFH4to0AKD362+z84YNG2I+XrNmjTIzM7Vr1y5Nnz49evvAgQOVnZ0dnw4BAH3SBb0GdPrP3GZkZMTc/swzz2jYsGEaP368li5dquPHj3f5OVpbWxWJRGI2AMBFwDg6efKk+chHPmKuvfbamNt/8IMfmA0bNpi9e/eap59+2owYMcLMnz+/y8+zbNkyI4mNjY2NrY9t4XD4rDniHEB33HGHGTNmjKmqqjrrfhs3bjSSTHl5eaf3t7S0mHA4HN2qqqq8LxobGxsb24Vv5wogq9eATlu8eLFeeeUVbdmyRSNHjjzrvgUFBZKk8vJyXXbZZWfcHwwGFQwGXdoAAPRiVgFkjNEXvvAFrV27VmVlZcrLyztnzZ49eyRJOTk5Tg0CAPomqwAqKSnRs88+q5dffllpaWmqqamRJIVCIaWmpqqiokLPPvusPvzhD+uSSy7R3r17dc8992j69OmaOHFiQr4AAEAvZfO6j7r4Od/q1auNMcYcOnTITJ8+3WRkZJhgMGjGjh1r7rvvvnP+HPBfhcNh7z+3ZGNjY2O78O1c3/sZRgoASAiGkQIAeiQCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwIseF0DGGN8tAADi4Fzfz3tcADU0NPhuAQAQB+f6fh4wPeySo6OjQ0eOHFFaWpoCgUDMfZFIRKNGjVJVVZXS09M9degf63AK63AK63AK63BKT1gHY4waGhqUm5urfv26vs7p3409nZd+/fpp5MiRZ90nPT39oj7BTmMdTmEdTmEdTmEdTvG9DqFQ6Jz79LgfwQEALg4EEADAi14VQMFgUMuWLVMwGPTdileswymswymswymswym9aR163JsQAAAXh151BQQA6DsIIACAFwQQAMALAggA4AUBBADwotcE0KpVq3TppZdqwIABKigo0I4dO3y31O2WL1+uQCAQs40bN853Wwm3ZcsW3XDDDcrNzVUgENC6deti7jfG6KGHHlJOTo5SU1NVWFiogwcP+mk2gc61DrfddtsZ58ecOXP8NJsgpaWluuaaa5SWlqbMzEzNmzdPBw4ciNmnpaVFJSUluuSSSzR48GDddNNNqq2t9dRxYpzPOsycOfOM8+GOO+7w1HHnekUA/fKXv9SSJUu0bNkyvf7665o0aZKKiopUV1fnu7Vu9573vEfV1dXR7X//9399t5RwTU1NmjRpklatWtXp/Y888ogef/xxPfnkk9q+fbsGDRqkoqIitbS0dHOniXWudZCkOXPmxJwfzz33XDd2mHibN29WSUmJtm3bpldffVXt7e2aPXu2mpqaovvcc889+tWvfqUXXnhBmzdv1pEjR3TjjTd67Dr+zmcdJGnhwoUx58MjjzziqeMumF5gypQppqSkJPrxyZMnTW5uriktLfXYVfdbtmyZmTRpku82vJJk1q5dG/24o6PDZGdnm29/+9vR2+rr600wGDTPPfechw67x7vXwRhjFixYYObOneulH1/q6uqMJLN582ZjzKnHPjk52bzwwgvRfd544w0jyWzdutVXmwn37nUwxpgZM2aYu+66y19T56HHXwG1tbVp165dKiwsjN7Wr18/FRYWauvWrR478+PgwYPKzc1Vfn6+PvWpT+nQoUO+W/KqsrJSNTU1MedHKBRSQUHBRXl+lJWVKTMzU1deeaUWLVqkY8eO+W4pocLhsCQpIyNDkrRr1y61t7fHnA/jxo3T6NGj+/T58O51OO2ZZ57RsGHDNH78eC1dulTHjx/30V6Xetw07Hc7evSoTp48qaysrJjbs7KytH//fk9d+VFQUKA1a9boyiuvVHV1tR5++GFdd9112rdvn9LS0ny350VNTY0kdXp+nL7vYjFnzhzdeOONysvLU0VFhb7yla+ouLhYW7duVVJSku/24q6jo0N33323rr32Wo0fP17SqfMhJSVFQ4YMidm3L58Pna2DJP37v/+7xowZo9zcXO3du1cPPPCADhw4oJdeesljt7F6fADhn4qLi6P/njhxogoKCjRmzBg9//zz+tznPuexM/QEt9xyS/TfEyZM0MSJE3XZZZeprKxMs2bN8thZYpSUlGjfvn0XxeugZ9PVOtx+++3Rf0+YMEE5OTmaNWuWKioqdNlll3V3m53q8T+CGzZsmJKSks54F0ttba2ys7M9ddUzDBkyRFdccYXKy8t9t+LN6XOA8+NM+fn5GjZsWJ88PxYvXqxXXnlFmzZtivn7YdnZ2Wpra1N9fX3M/n31fOhqHTpTUFAgST3qfOjxAZSSkqLJkydr48aN0ds6Ojq0ceNGTZ061WNn/jU2NqqiokI5OTm+W/EmLy9P2dnZMedHJBLR9u3bL/rz4/Dhwzp27FifOj+MMVq8eLHWrl2r1157TXl5eTH3T548WcnJyTHnw4EDB3To0KE+dT6cax06s2fPHknqWeeD73dBnI9f/OIXJhgMmjVr1pi//OUv5vbbbzdDhgwxNTU1vlvrVl/60pdMWVmZqaysNP/3f/9nCgsLzbBhw0xdXZ3v1hKqoaHB7N692+zevdtIMo8++qjZvXu3eeutt4wxxnzrW98yQ4YMMS+//LLZu3evmTt3rsnLyzPNzc2eO4+vs61DQ0ODuffee83WrVtNZWWl+e1vf2ve//73m8svv9y0tLT4bj1uFi1aZEKhkCkrKzPV1dXR7fjx49F97rjjDjN69Gjz2muvmZ07d5qpU6eaqVOneuw6/s61DuXl5eZrX/ua2blzp6msrDQvv/yyyc/PN9OnT/fceaxeEUDGGPO9733PjB492qSkpJgpU6aYbdu2+W6p2918880mJyfHpKSkmBEjRpibb77ZlJeX+24r4TZt2mQknbEtWLDAGHPqrdgPPvigycrKMsFg0MyaNcscOHDAb9MJcLZ1OH78uJk9e7YZPny4SU5ONmPGjDELFy7sc/9J6+zrl2RWr14d3ae5udnceeedZujQoWbgwIFm/vz5prq62l/TCXCudTh06JCZPn26ycjIMMFg0IwdO9bcd999JhwO+238Xfh7QAAAL3r8a0AAgL6JAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8+H8oEH+/Vq/vUwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get opto loader\n",
    "random_opto_loader = generate_opto_loader(random_train_loader, opto_category, plot=True)\n",
    "reward_opto_loader = generate_opto_loader(reward_train_loader, opto_category, plot=True)\n",
    "punish_opto_loader = generate_opto_loader(punish_train_loader, opto_category, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af6981",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "c182daec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsUUlEQVR4nO3deXRUZZ7/8U8lIQuQhbCFKCEIiC2yhGZRFAh2ujFGxGVGBQYRPWKPICJqE1qRxIWAx1ZcGGlxFJ0joj0K7ciIC5Lgwm5wbBGEmEhUNhusGJAihOf3h6Z+FiQhVbn1VCq8X+fcI/fe597n+00qycdbt6pcxhgjAAAASyJCXQAAADi9ED4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWBUV6gJOdPz4cX333XeKj4+Xy+UKdTkAAKABjDH68ccflZqaqoiI+q9tNLnw8d1336lz586hLgMAAASgvLxcZ555Zr1jmlz4iI+Pl/Rz8QkJCSGuBgAANERFRYU6d+7s/TtenyYXPmqeaklISCB8AAAQZhpyywQ3nAIAAKsIHwAAwCrCBwAAsKrJ3fMBAGhaqqurVVVVFeoy0ARERkYqKiqq0W+FQfgAANSpsrJS33zzjYwxoS4FTUTLli3VqVMnRUdHB3wOwgcAoFbV1dX65ptv1LJlS7Vv3543fjzNGWN09OhR7d+/X6WlperRo8cp30ysLoQPAECtqqqqZIxR+/btFRcXF+py0ATExcWpRYsW+vrrr3X06FHFxsYGdB5uOAUA1IsrHvi1QK92+JzDgToAAAAajPABAACs4p4PAIBf0nNXWJ2vbG6O1fkQfFz5AAAAVhE+AACntaNHj4a6hNMO4QMA0KxkZmZqypQpmjJlihITE9WuXTvNmjXL+0Zp6enpeuCBB3T99dcrISFBkyZNkiR9+OGHGjp0qOLi4tS5c2dNnTpVhw4dOuV8Tz31lM477zzv+vLly+VyubRw4ULvtqysLN17770Odxq+uOcDcEBtz4EH43nqE+fhuXCL8hJPWHcHf45gzXMaeOGFF3TTTTdpw4YN2rRpkyZNmqS0tDTdfPPNkqRHHnlE9913n2bPni1JKikp0SWXXKIHH3xQzz33nPbv3+8NMM8//3y9cw0fPlxTp07V/v371b59exUVFaldu3YqLCzUH//4R1VVVWnt2rXKzc0Net/hgisfAIBmp3PnznrsscfUs2dPjRs3Trfddpsee+wx7/6LL75Yd955p7p166Zu3bqpoKBA48aN07Rp09SjRw8NGTJETzzxhF588UUdOXKk3rnOO+88JScnq6ioSJJUWFioO++807u+YcMGVVVVaciQIcFrOMwQPgAAzc7555/v8+ZoF1xwgXbs2KHq6mpJ0oABA3zGf/rpp1q8eLFat27tXUaOHKnjx4+rtLS03rlcLpeGDRumwsJC/fDDD9q6datuvfVWeTwebdu2TUVFRRo4cKBatmzpfKNhiqddAACnnVatWvmsV1ZW6pZbbtHUqVNPGpuWlnbK82VmZuqZZ57RBx98oIyMDCUkJHgDSVFRkYYPH+5Y7c0B4QMA0OysX7/eZ33dunXq0aOHIiMjax3fv39/bd26Vd27dw9ovuHDh2vatGn629/+pszMTEk/B5L33ntPH330ke68886Azttc8bQLAKDZ2bVrl6ZPn67t27fr5Zdf1pNPPqnbb7+9zvEzZszQxx9/rClTpmjLli3asWOH/v73v2vKlCkNmq9Pnz5q06aNlixZ4hM+li9fLo/HowsvvNCJtpoNrnwAAPwSDq+yuv766/XTTz9p0KBBioyM1O233+59SW1t+vTpo6KiIt1zzz0aOnSojDHq1q2brr322gbN53K5NHToUK1YsUIXXXSR95wJCQnq2bPnSU/znO4IHwCAZqdFixaaP3++nn766ZP2lZWV1XrMwIED9c477wQ85/Lly33WIyIidODAgYDP15zxtAsAALCK8AEAQD0++OADn5fgnrjAfzztAgBoVgoLCx0934ABA7RlyxZHz3m6I3wAAFCPuLi4gF+Ci9rxtAsAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsMrv8LFmzRqNGjVKqampcrlcJ72jmyR98cUXuvzyy5WYmKhWrVpp4MCB2rVrlxP1AgBQL2OMJk2apOTkZLlcLiUlJWnatGmOzpGXl6d+/fo5es6GyszM9Lufuv5eh4rfL7U9dOiQ+vbtqxtvvFFXXXXVSftLSkp00UUX6aabblJ+fr4SEhL0+eefKzY21pGCAQAhlpdoeT63X8NXrlypxYsXq7CwUGeddZYiIiIUFxcXpOLse/3119WiRQtHz1lYWKgRI0bo4MGDSkpKcvTctfE7fGRnZys7O7vO/ffcc48uvfRSPfzww95t3bp1C6w6AAD8VFJSok6dOmnIkCGhLiUokpOTQ11Cozl6z8fx48e1YsUKnX322Ro5cqQ6dOigwYMH13upx+PxqKKiwmcBACAQN9xwg2677Tbt2rVLLpdL6enpJz1NkZ6erjlz5ujGG29UfHy80tLS9Mwzz/icZ8aMGTr77LPVsmVLnXXWWZo1a5aqqqr8rucf//iHIiIitH//fknSgQMHFBERoeuuu8475sEHH/R+Em7NMdnZ2WrdurU6duyo8ePH6/vvv/fuP7Gf3bt3KycnR3FxceratauWLFmi9PR0zZ8/36eW77//XldeeaVatmypHj166I033pD08wftjRgxQpLUpk0buVwu3XDDDX736g9Hw8e+fftUWVmpuXPn6pJLLtE777yjK6+8UldddZWKiopqPaagoECJiYnepXPnzk6WBJye8hJPXkIxrx/Sc1f4LDacOKeteRE8jz/+uO6//36deeaZ2r17tzZu3FjruL/85S8aMGCAiouLdeutt+rf//3ftX37du/++Ph4LV68WFu3btXjjz+uRYsW6bHHHvO7nl69eqlt27bev4EffPCBz7okFRUVKTMzU5L0ww8/6OKLL1ZGRoY2bdqklStXau/evbrmmmvqnOP666/Xd999p8LCQr322mt65plntG/fvpPG5efn65prrtH//d//6dJLL9W4ceN04MABde7cWa+99pokafv27dq9e7cef/xxv3v1h+NXPiRp9OjRuuOOO9SvXz/l5ubqsssu08KFC2s9ZubMmXK73d6lvLzcyZIAAKeRxMRExcfHKzIyUikpKWrfvn2t4y699FLdeuut6t69u2bMmKF27dpp9erV3v333nuvhgwZovT0dI0aNUp33XWXXn31Vb/rcblcGjZsmPfzZgoLCzVx4kR5PB5t27ZNVVVV+vjjjzV8+HBJ0lNPPaWMjAzNmTNH55xzjjIyMvTcc89p9erV+vLLL086/7Zt2/Tee+9p0aJFGjx4sPr3769nn31WP/3000ljb7jhBo0ZM0bdu3fXnDlzVFlZqQ0bNigyMtL7VE6HDh2UkpKixMTg/g+Lo5/t0q5dO0VFRencc8/12f6b3/xGH374Ya3HxMTEKCYmxskyAACoV58+fbz/drlcSklJ8bla8Morr+iJJ55QSUmJKisrdezYMSUkJAQ01/Dhw71P6xQVFWnOnDn68ssvVVhYqAMHDqiqqkoXXnihJOnTTz/V6tWra/203JKSEp199tk+27Zv366oqCj179/fu6179+5q06ZNvT23atVKCQkJtV4hscHR8BEdHa2BAwf6XLqSpC+//FJdunRxcioAAAJ24qtFXC6X9+r92rVrNW7cOOXn52vkyJFKTEzU0qVL9Ze//CWguWru0dixY4e2bt2qiy66SNu2bVNhYaEOHjyoAQMGqGXLlpKkyspKjRo1SvPmzTvpPJ06dQpo/hr19Wyb3+GjsrJSO3fu9K6XlpZqy5YtSk5OVlpamu6++25de+21GjZsmEaMGKGVK1fqf/7nfxz/iGMAAILh448/VpcuXXTPPfd4t3399dcBn693795q06aNHnzwQfXr10+tW7dWZmam5s2bp4MHD3rv95Ck/v3767XXXlN6erqiok79J7pnz546duyYiouL9dvf/laStHPnTh08eNCvGqOjoyVJ1dXVfh0XKL/v+di0aZMyMjKUkZEhSZo+fboyMjJ03333SZKuvPJKLVy4UA8//LB69+6tZ599Vq+99prPnbwAADRVPXr00K5du7R06VKVlJToiSee0LJlywI+X819Hy+99JI3aPTp00cej0erVq3y3u8hSZMnT9aBAwc0ZswYbdy4USUlJXr77bc1ceLEWoPBOeeco6ysLE2aNEkbNmxQcXGxJk2apLi4OLlcrgbX2KVLF7lcLr355pvav3+/KisrA+63Ify+8pGZmSljTL1jbrzxRt14440BFwUAaML8fNOvcHP55Zfrjjvu0JQpU+TxeJSTk6NZs2YpLy8v4HMOHz5cy5cv94aPiIgIDRs2TCtWrPDe7yFJqamp+uijjzRjxgz94Q9/kMfjUZcuXXTJJZcoIqL26wUvvviibrrpJg0bNkwpKSkqKCjw+809zzjjDOXn5ys3N1cTJ07U9ddfr8WLFwfc76m4zKmShGUVFRVKTEyU2+0O+OYewLbaXqJZNjcn6PPUOUdtL3O18QfjxHn9mLPBvTnIr+9bI3prsFB93+pw5MgRlZaWqmvXrrxLdRj55ptv1LlzZ7333nv63e9+5/j563pc+PP329EbTgEAgF3vv/++Kisr1bt3b+3evVt/+tOflJ6ermHDhoW6tDoRPgAAaITaXhZb46233tLQoUODOn9VVZX+/Oc/66uvvlJ8fLyGDBmil156yfHPf3ES4QMAgEbYsmVLnfvOOOOMoM8/cuRIjRw5MujzOInwAQBAI3Tv3j3UJYQdR99eHQDQ/DSx1yUgxJx4PBA+AAC1ioyMlCQdPXo0xJWgKTl8+LCkk98x1R887QIAqFVUVJRatmyp/fv3q0WLFnW+zwROD8YYHT58WPv27VNSUpI3nAaC8AEAqJXL5VKnTp1UWlraqLcXR/OSlJSklJSURp2D8AEAqFN0dLR69OjBUy+Q9PNTLY254lGD8AEAqFdERATvcApH8QQeAACwivABAACsInwAAACruOcDQPDU80mwtX2iLIDTA1c+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABY5Xf4WLNmjUaNGqXU1FS5XC4tX768zrF//OMf5XK5NH/+/EaUCAAAmhO/w8ehQ4fUt29fLViwoN5xy5Yt07p165SamhpwcQAAoPmJ8veA7OxsZWdn1zvm22+/1W233aa3335bOTk5ARcHAACaH7/Dx6kcP35c48eP1913361evXqdcrzH45HH4/GuV1RUOF0SAABoQhwPH/PmzVNUVJSmTp3aoPEFBQXKz893ugygWSuLHfvzP/J+2ZDnbvCx6bkr6j/33F+uVuYl+u7wYw4bauujztpPFMDXqyy2wYcAOAVHX+2yefNmPf7441q8eLFcLleDjpk5c6bcbrd3KS8vd7IkAADQxDgaPj744APt27dPaWlpioqKUlRUlL7++mvdeeedSk9Pr/WYmJgYJSQk+CwAAKD5cvRpl/HjxysrK8tn28iRIzV+/HhNnDjRyakAAECY8jt8VFZWaufOnd710tJSbdmyRcnJyUpLS1Pbtm19xrdo0UIpKSnq2bNn46sFAABhz+/wsWnTJo0YMcK7Pn36dEnShAkTtHjxYscKAwAAzZPf4SMzM1PGmAaPLysr83cKAADQjPHZLgAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsCoq1AUAsCQvUZJUFvvzavqRJSEsJrTSc1eEugTgtMaVDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVX6HjzVr1mjUqFFKTU2Vy+XS8uXLvfuqqqo0Y8YM9e7dW61atVJqaqquv/56fffdd07WDAAAwpjf4ePQoUPq27evFixYcNK+w4cP65NPPtGsWbP0ySef6PXXX9f27dt1+eWXO1IsAAAIf1H+HpCdna3s7Oxa9yUmJurdd9/12fbUU09p0KBB2rVrl9LS0gKrEgAANBt+hw9/ud1uuVwuJSUl1brf4/HI4/F41ysqKoJdEgAACKGgho8jR45oxowZGjNmjBISEmodU1BQoPz8/GCWAYSF9NwVPutlc3OCOl9Z7Fjf+Y8sCep8zV5eYi3b3PbrAMJA0F7tUlVVpWuuuUbGGD399NN1jps5c6bcbrd3KS8vD1ZJAACgCQjKlY+a4PH111/r/fffr/OqhyTFxMQoJiYmGGUAAIAmyPHwURM8duzYodWrV6tt27ZOTwEAAMKY3+GjsrJSO3fu9K6XlpZqy5YtSk5OVqdOnfQv//Iv+uSTT/Tmm2+qurpae/bskSQlJycrOjraucoBAEBY8jt8bNq0SSNGjPCuT58+XZI0YcIE5eXl6Y033pAk9evXz+e41atXKzMzM/BKAQBAs+B3+MjMzJQxps799e0DAADgs10AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGBVVKgLAMJKXqLPavqRJT7rZbFjfzW25r/u4NYkKT13xc/zz80J+ly/nq9GY+b1+Zrp5K/piV9zG1/PX89bFhv8qbzfv9rmquMxZ+t7DQQDVz4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjld/hYs2aNRo0apdTUVLlcLi1fvtxnvzFG9913nzp16qS4uDhlZWVpx44dTtULAADCnN/h49ChQ+rbt68WLFhQ6/6HH35YTzzxhBYuXKj169erVatWGjlypI4cOdLoYgEAQPiL8veA7OxsZWdn17rPGKP58+fr3nvv1ejRoyVJL774ojp27Kjly5fruuuua1y1AAAg7Dl6z0dpaan27NmjrKws77bExEQNHjxYa9eurfUYj8ejiooKnwUAADRffl/5qM+ePXskSR07dvTZ3rFjR+++ExUUFCg/P9/JMgAf6bkrfNbL5uac1nWcSk2dZbG1b/+1stixP/8jr4Enz0v81bGB1VFzjlqPb2gdofKr/n9ed5/edeC0FfJXu8ycOVNut9u7lJeXh7okAAAQRI6Gj5SUFEnS3r17fbbv3bvXu+9EMTExSkhI8FkAAEDz5Wj46Nq1q1JSUrRq1SrvtoqKCq1fv14XXHCBk1MBAIAw5fc9H5WVldq5c6d3vbS0VFu2bFFycrLS0tI0bdo0Pfjgg+rRo4e6du2qWbNmKTU1VVdccYWTdQMAgDDld/jYtGmTRowY4V2fPn26JGnChAlavHix/vSnP+nQoUOaNGmSfvjhB1100UVauXKlYmNPcXcZAAA4LfgdPjIzM2WMqXO/y+XS/fffr/vvv79RhQEAgOYp5K92AQAApxfCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsCoq1AUAzV167or6B+QlSpLKYn8Zf2TJSceVxY71GXOimv3KC7hM/KLm617X19rrl+9bg841N6fec5xyLqCZ4coHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrHA8f1dXVmjVrlrp27aq4uDh169ZNDzzwgIwxTk8FAADCUJTTJ5w3b56efvppvfDCC+rVq5c2bdqkiRMnKjExUVOnTnV6OgAAEGYcDx8ff/yxRo8erZycHElSenq6Xn75ZW3YsMHpqQAAQBhy/GmXIUOGaNWqVfryyy8lSZ9++qk+/PBDZWdnOz0VAAAIQ45f+cjNzVVFRYXOOeccRUZGqrq6Wg899JDGjRtX63iPxyOPx+Ndr6iocLokAADQhDgePl599VW99NJLWrJkiXr16qUtW7Zo2rRpSk1N1YQJE04aX1BQoPz8fKfLAKwoix0b+DF5zp3TCaGa90Q26qhtjvQjS4I3T57jpwbCmuNPu9x9993Kzc3Vddddp969e2v8+PG64447VFBQUOv4mTNnyu12e5fy8nKnSwIAAE2I41c+Dh8+rIgI30wTGRmp48eP1zo+JiZGMTExTpcBAACaKMfDx6hRo/TQQw8pLS1NvXr1UnFxsR599FHdeOONTk8FAADCkOPh48knn9SsWbN06623at++fUpNTdUtt9yi++67z+mpAABAGHI8fMTHx2v+/PmaP3++06cGAADNAJ/tAgAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq6JCXQAQaum5K7z/Losd67szz+0zpizW//OfdE40SeHyffLWmffLhpMeo7X08csYoKngygcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwKqghI9vv/1W//Zv/6a2bdsqLi5OvXv31qZNm4IxFQAACDNRTp/w4MGDuvDCCzVixAi99dZbat++vXbs2KE2bdo4PRUAAAhDjoePefPmqXPnznr++ee927p27er0NAAAIEw5/rTLG2+8oQEDBuhf//Vf1aFDB2VkZGjRokV1jvd4PKqoqPBZAABA8+X4lY+vvvpKTz/9tKZPn64///nP2rhxo6ZOnaro6GhNmDDhpPEFBQXKz893ugycjvIST1h3h6aOZqosdmyoS0CQpeeu8Fkvm5sTokrQ3Dl+5eP48ePq37+/5syZo4yMDE2aNEk333yzFi5cWOv4mTNnyu12e5fy8nKnSwIAAE2I4+GjU6dOOvfcc322/eY3v9GuXbtqHR8TE6OEhASfBQAANF+Oh48LL7xQ27dv99n25ZdfqkuXLk5PBQAAwpDj4eOOO+7QunXrNGfOHO3cuVNLlizRM888o8mTJzs9FQAACEOOh4+BAwdq2bJlevnll3XeeefpgQce0Pz58zVu3DinpwIAAGHI8Ve7SNJll12myy67LBinBgAAYY7PdgEAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFVRoS4AaJC8RJ/V9CNLJEllc3NCUQ1w+uFnEA7iygcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKuCHj7mzp0rl8uladOmBXsqAAAQBoIaPjZu3Ki//vWv6tOnTzCnAQAAYSRo4aOyslLjxo3TokWL1KZNm2BNAwAAwkzQwsfkyZOVk5OjrKysesd5PB5VVFT4LAAAoPmKCsZJly5dqk8++UQbN2485diCggLl5+cHoww0Mem5K3zWy+bmhKiShjuxZqDJyUuUJJXFhriOU/mlTt9tbvt1oElw/MpHeXm5br/9dr300kuKjT31T8PMmTPldru9S3l5udMlAQCAJsTxKx+bN2/Wvn371L9/f++26upqrVmzRk899ZQ8Ho8iIyO9+2JiYhQTE+N0GQAAoIlyPHz87ne/02effeazbeLEiTrnnHM0Y8YMn+ABAABOP46Hj/j4eJ133nk+21q1aqW2bduetB0AAJx+eIdTAABgVVBe7XKiwsJCG9MAAIAwwJUPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABY5TLGmFAX8WsVFRVKTEyU2+1WQkJCqMuBg9JzV5xyTNncnJ//kZcY5GoaJv3IEp/1stixIaoECNypHscn7q9NQ4/x62c4z33qMQgb/vz95soHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqx8NHQUGBBg4cqPj4eHXo0EFXXHGFtm/f7vQ0AAAgTDkePoqKijR58mStW7dO7777rqqqqvSHP/xBhw4dcnoqAAAQhqKcPuHKlSt91hcvXqwOHTpo8+bNGjZsmNPTAQCAMON4+DiR2+2WJCUnJ9e63+PxyOPxeNcrKiqCXRIAAAihoIaP48ePa9q0abrwwgt13nnn1TqmoKBA+fn5wSwjPOUl1rLNHfI60o8s8f67bG6O777cFT7rJ+73bo8de9K2mvPWnKMs1v9Sg6G2WoFw4+/jOJDHvfeYPL8P/f9O/L134u+8Wn4v1vzuqOv3TTCd+DsvVHWEo6C+2mXy5Mn6xz/+oaVLl9Y5ZubMmXK73d6lvLw8mCUBAIAQC9qVjylTpujNN9/UmjVrdOaZZ9Y5LiYmRjExMcEqAwAANDGOhw9jjG677TYtW7ZMhYWF6tq1q9NTAACAMOZ4+Jg8ebKWLFmiv//974qPj9eePXskSYmJiYqLi3N6OgAAEGYcv+fj6aefltvtVmZmpjp16uRdXnnlFaenAgAAYSgoT7sAAADUhc92AQAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVVGhLsC6vMQT1t1Bmyo9d8VJ28rm5jT6fKc6x4nz+ox3sH9vPbFjf/nvL9uPLKl3P4CmpeZn1LoTfx/5u98Pjv5e/GX8r3+n1fzeazCLf4ukU/QfAlz5AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgVdDCx4IFC5Senq7Y2FgNHjxYGzZsCNZUAAAgjAQlfLzyyiuaPn26Zs+erU8++UR9+/bVyJEjtW/fvmBMBwAAwkhQwsejjz6qm2++WRMnTtS5556rhQsXqmXLlnruueeCMR0AAAgjUU6f8OjRo9q8ebNmzpzp3RYREaGsrCytXbv2pPEej0cej8e77na7JUkVFRVOl/bLhMZ3PVjzSDruOXzStgb3dWKdvzrfqc5x4rw+4/3t/4Txtfbkqn/MiftrE8gxAEKnqfzMNoXfi/7UEfCcjVRv/w6pOacxDXgsGId9++23RpL5+OOPfbbffffdZtCgQSeNnz17tpHEwsLCwsLC0gyW8vLyU2YFx698+GvmzJmaPn26d/348eM6cOCA2rZtK5fLFfT5Kyoq1LlzZ5WXlyshISHo89lEb+GJ3sJXc+6P3sKTzd6MMfrxxx+Vmpp6yrGOh4927dopMjJSe/fu9dm+d+9epaSknDQ+JiZGMTExPtuSkpKcLuuUEhISmt2Drga9hSd6C1/NuT96C0+2ektMTGzQOMdvOI2OjtZvf/tbrVq1yrvt+PHjWrVqlS644AKnpwMAAGEmKE+7TJ8+XRMmTNCAAQM0aNAgzZ8/X4cOHdLEiRODMR0AAAgjQQkf1157rfbv36/77rtPe/bsUb9+/bRy5Up17NgxGNM1SkxMjGbPnn3SUz/NAb2FJ3oLX825P3oLT021N5cxDXlNDAAAgDP4bBcAAGAV4QMAAFhF+AAAAFYRPgAAgFWnZfg4cOCAxo0bp4SEBCUlJemmm25SZWVlg441xig7O1sul0vLly8PbqEBCKS3W265Rd26dVNcXJzat2+v0aNHa9u2bZYqbjh/eztw4IBuu+029ezZU3FxcUpLS9PUqVO9nx/UlATyfXvmmWeUmZmphIQEuVwu/fDDD3aKPYUFCxYoPT1dsbGxGjx4sDZs2FDv+L/97W8655xzFBsbq969e+t///d/LVUaGH/6+/zzz3X11VcrPT1dLpdL8+fPt1doAPzpbdGiRRo6dKjatGmjNm3aKCsr65Tf61Dyp7fXX39dAwYMUFJSklq1aqV+/frpv/7rvyxW6x9/f+ZqLF26VC6XS1dccUVwC6yNIx/oEmYuueQS07dvX7Nu3TrzwQcfmO7du5sxY8Y06NhHH33UZGdnG0lm2bJlwS00AIH09te//tUUFRWZ0tJSs3nzZjNq1CjTuXNnc+zYMUtVN4y/vX322WfmqquuMm+88YbZuXOnWbVqlenRo4e5+uqrLVbdMIF83x577DFTUFBgCgoKjCRz8OBBO8XWY+nSpSY6Oto899xz5vPPPzc333yzSUpKMnv37q11/EcffWQiIyPNww8/bLZu3Wruvfde06JFC/PZZ59Zrrxh/O1vw4YN5q677jIvv/yySUlJMY899pjdgv3gb29jx441CxYsMMXFxeaLL74wN9xwg0lMTDTffPON5cpPzd/eVq9ebV5//XWzdetWs3PnTjN//nwTGRlpVq5cabnyU/O3txqlpaXmjDPOMEOHDjWjR4+2U+yvnHbhY+vWrUaS2bhxo3fbW2+9ZVwul/n222/rPba4uNicccYZZvfu3U0yfDSmt1/79NNPjSSzc+fOYJQZEKd6e/XVV010dLSpqqoKRpkBaWxvq1evbjLhY9CgQWby5Mne9erqapOammoKCgpqHX/NNdeYnJwcn22DBw82t9xyS1DrDJS//f1aly5dmnT4aExvxhhz7NgxEx8fb1544YVglRiwxvZmjDEZGRnm3nvvDUZ5jRJIb8eOHTNDhgwxzz77rJkwYUJIwsdp97TL2rVrlZSUpAEDBni3ZWVlKSIiQuvXr6/zuMOHD2vs2LFasGBBrZ9R0xQE2tuvHTp0SM8//7y6du2qzp07B6tUvznRmyS53W4lJCQoKirkn6no5VRvoXb06FFt3rxZWVlZ3m0RERHKysrS2rVraz1m7dq1PuMlaeTIkXWOD6VA+gsXTvR2+PBhVVVVKTk5OVhlBqSxvRljtGrVKm3fvl3Dhg0LZql+C7S3+++/Xx06dNBNN91ko8xanXbhY8+ePerQoYPPtqioKCUnJ2vPnj11HnfHHXdoyJAhGj16dLBLDFigvUnSf/zHf6h169Zq3bq13nrrLb377ruKjo4OZrl+aUxvNb7//ns98MADmjRpUjBKDJgTvTUF33//vaqrq096J+OOHTvW2ceePXv8Gh9KgfQXLpzobcaMGUpNTT0pTIZaoL253W61bt1a0dHRysnJ0ZNPPqnf//73wS7XL4H09uGHH+o///M/tWjRIhsl1qnZhI/c3Fy5XK56l0BvonzjjTf0/vvvh+xmsWD2VmPcuHEqLi5WUVGRzj77bF1zzTU6cuSIQx3UzUZv0s8fK52Tk6Nzzz1XeXl5jS+8AWz1BoTa3LlztXTpUi1btkyxsbGhLscR8fHx2rJlizZu3KiHHnpI06dPV2FhYajLapQff/xR48eP16JFi9SuXbuQ1tJ0rj030p133qkbbrih3jFnnXWWUlJStG/fPp/tx44d04EDB+p8OuX9999XSUmJkpKSfLZfffXVGjp0aNAfkMHsrUZiYqISExPVo0cPnX/++WrTpo2WLVumMWPGNLb8etno7ccff9Qll1yi+Ph4LVu2TC1atGhs2Q1io7empF27doqMjNTevXt9tu/du7fOPlJSUvwaH0qB9BcuGtPbI488orlz5+q9995Tnz59gllmQALtLSIiQt27d5ck9evXT1988YUKCgqUmZkZzHL94m9vJSUlKisr06hRo7zbjh8/Lunnq63bt29Xt27dglt0Det3mYRYzc19mzZt8m57++236725b/fu3eazzz7zWSSZxx9/3Hz11Ve2Sj+lQHqrzZEjR0xcXJx5/vnng1BlYALtze12m/PPP98MHz7cHDp0yEapfmvs962p3XA6ZcoU73p1dbU544wz6r3h9LLLLvPZdsEFFzTpG0796e/XwuGGU397mzdvnklISDBr1661UWLAGvN9qzFx4kQzfPjwIFTXOP709tNPP530t2z06NHm4osvNp999pnxeDzW6j7twocxP7+sMSMjw6xfv958+OGHpkePHj4va/zmm29Mz549zfr16+s8h5rgq12M8b+3kpISM2fOHLNp0ybz9ddfm48++siMGjXKJCcnn/KlWrb525vb7TaDBw82vXv3Njt37jS7d+/2Lk3xZcT+PiZ3795tiouLzaJFi4wks2bNGlNcXGz++c9/hqIFY8zPL/uLiYkxixcvNlu3bjWTJk0ySUlJZs+ePcYYY8aPH29yc3O94z/66CMTFRVlHnnkEfPFF1+Y2bNnN/mX2vrTn8fjMcXFxaa4uNh06tTJ3HXXXaa4uNjs2LEjVC3Uyd/e5s6da6Kjo81///d/+/xs/fjjj6FqoU7+9jZnzhzzzjvvmJKSErN161bzyCOPmKioKLNo0aJQtVAnf3s7Uahe7XJaho9//vOfZsyYMaZ169YmISHBTJw40ecHprS01Egyq1evrvMcTTV8+Nvbt99+a7Kzs02HDh1MixYtzJlnnmnGjh1rtm3bFqIO6uZvbzVXBGpbSktLQ9NEHQJ5TM6ePbvW3kJ9xerJJ580aWlpJjo62gwaNMisW7fOu2/48OFmwoQJPuNfffVVc/bZZ5vo6GjTq1cvs2LFCssV+8ef/mq+bycuTfH/oI3xr7cuXbrU2tvs2bPtF94A/vR2zz33mO7du5vY2FjTpk0bc8EFF5ilS5eGoOqG8fdn7tdCFT5cxhgT7Kd2AAAAajSbV7sAAIDwQPgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABg1f8DpesOA3CHJEcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Corelease_Model(in_features=784, h1=512, h2=512, out_features=1).to(device)\n",
    "model\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, TopKLinear):  # Check if it's a TopKLinear layer\n",
    "        plt.hist(module.pre_w.detach().cpu().numpy().flatten(), bins=100)\n",
    "        plt.hist(module.final_weight.detach().cpu().numpy().flatten(), bins=100)\n",
    "        plt.legend([\"pre_w\", \"final_weight\"])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "39753b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network dictionary\n",
    "network_dict = {\"network_type\": [\"MLP\", \"MLP\", \"MLP\", \"MLP\", \"MLP\"], \n",
    "                \"EP_LHb\": [\"random\", \"random\", \"dales-law\", \"dales-law\", \"dales-law\"],\n",
    "                \"LHb_DAN\": [\"real\", \"mixed\", \"real\", \"real\", \"real\"], \n",
    "                \"update_methods\": [\"corelease\", \"corelease\", \"fixed-sign\", \"fixed-sign\", \"fixed-sign\"], \n",
    "                \"split_EI\": [\"no_split\", \"no_split\", \"split_EP_only\", \"split_EP_LHb\", \"combine_EI\"]}\n",
    "\n",
    "training_loss_summary, test_accuracy_summary, val_accuracy_summary, initial_params_summary, trained_params_summary, params_summary = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "num_networks = 1\n",
    "\n",
    "# initialize loss/accuracy variables\n",
    "epochs = 5\n",
    "T_max = 5\n",
    "lr = 0.01\n",
    "\n",
    "# layer neuron num\n",
    "in_features = 784\n",
    "h1_ls = [512]\n",
    "h2_ls = [512]\n",
    "\n",
    "opto_neuron_percent = 0.2\n",
    "\n",
    "# Training sessions\n",
    "random_learning = True # always true\n",
    "reward_learning = True\n",
    "punish_learning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "5c01d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "optimizer = LogAdam(params=model.parameters())\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "9041ca48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP.pre_w: Positive: 49.90%; Negative: 50.10%; Zero: 0.00%\n",
      "EP.sign_matrix: Positive: 50.09%; Negative: 49.91%; Zero: 0.00%\n",
      "EP.final_weight: Positive: 50.09%; Negative: 49.91%; Zero: 0.00%\n",
      "LHb.pre_w: Positive: 49.99%; Negative: 50.01%; Zero: 0.00%\n",
      "LHb.sign_matrix: Positive: 50.01%; Negative: 49.99%; Zero: 0.00%\n",
      "LHb.final_weight: Positive: 50.01%; Negative: 49.99%; Zero: 0.00%\n",
      "DAN.pre_w: Positive: 52.54%; Negative: 47.46%; Zero: 0.00%\n",
      "DAN.sign_matrix: Positive: 0.00%; Negative: 100.00%; Zero: 0.00%\n",
      "DAN.final_weight: Positive: 0.00%; Negative: 100.00%; Zero: 0.00%\n",
      "training MLP_random_real_corelease_no_split_1 (512 EP neurons, 512 LHb neurons), random-train, with opto, loss:MSE\n",
      "False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogAdam' object has no attribute 'named_groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[523], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, random-train, with opto, loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_function\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_test_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_ls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopto_category\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopto_on\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# record and store trained params\u001b[39;00m\n\u001b[1;32m     81\u001b[0m trained_params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrecord_params()\n",
      "Cell \u001b[0;32mIn[484], line 52\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, val_loader, epochs, params_ls, opto_category, opto_on, loss_function, grad_clip_value)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     model\u001b[38;5;241m.\u001b[39mgradient_history[neuron_idx]\u001b[38;5;241m.\u001b[39mappend(param\u001b[38;5;241m.\u001b[39mgrad[:, neuron_idx])\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Update signs dynamically after gradient computation\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m~/miniconda3/envs/eplhbmodel/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eplhbmodel/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[521], line 52\u001b[0m, in \u001b[0;36mLogAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     49\u001b[0m             step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     50\u001b[0m             param\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m---> 52\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_groups\u001b[49m():\n\u001b[1;32m     53\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogAdam' object has no attribute 'named_groups'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for i in range(0, 1):\n",
    "    for h1_count in range(len(h1_ls)):\n",
    "        for h2_count in range(len(h2_ls)):\n",
    "            for num in range(1,num_networks+1):\n",
    "                # initialize results storage\n",
    "                training_loss = []\n",
    "                val_accuracy = []\n",
    "                test_accuracy = []\n",
    "                params_ls = []\n",
    "\n",
    "                # network conditions\n",
    "                network_type = network_dict[\"network_type\"][i]\n",
    "\n",
    "                # dales-law\n",
    "                EP_LHb = network_dict[\"EP_LHb\"][i]\n",
    "                if EP_LHb == \"dales-law\":\n",
    "                    dales_law = True\n",
    "                else:\n",
    "                    dales_law = False\n",
    "\n",
    "                # Real LHb to DAN\n",
    "                LHb_DAN = network_dict[\"LHb_DAN\"][i]\n",
    "                if LHb_DAN == \"real\":\n",
    "                    real = True\n",
    "                else:\n",
    "                    real = False\n",
    "\n",
    "                # Update method\n",
    "                update_methods = network_dict[\"update_methods\"][i]\n",
    "                if update_methods == \"fixed-sign\":\n",
    "                    fixed_sign = True\n",
    "                else:\n",
    "                    fixed_sign = False\n",
    "\n",
    "                # split EI\n",
    "                split_EI = network_dict[\"split_EI\"][i]\n",
    "                if split_EI == \"combine_EI\":\n",
    "                    combine_EI = True\n",
    "                elif split_EI == \"split_EP_only\":\n",
    "                    split_EP_only = True\n",
    "                else:\n",
    "                    combine_EI = False\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Instantiate model\n",
    "                torch.manual_seed(1000)\n",
    "                model = Corelease_Model(in_features=in_features, h1=h1_ls[h1_count], h2=h2_ls[h2_count], \n",
    "                                        out_features=out_features, real = real, combine_EI = combine_EI, \n",
    "                                        opto_neuron_percent = opto_neuron_percent, batch_size = batch_size \n",
    "                                        ).to(device)\n",
    "\n",
    "                # network name\n",
    "                network_name = network_type+'_'+EP_LHb+'_'+LHb_DAN+'_'+update_methods+'_' + split_EI + '_' + str(num) + ' ('+ str(h1_ls[h1_count])+' EP neurons, ' + str(h2_ls[h2_count])+' LHb neurons)'\n",
    "                # adam optimizer\n",
    "                base_optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                optimizer = LogAdam(params=model.parameters())\n",
    "                # Learning rate scheduler\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max) \n",
    "                \n",
    "                \n",
    "                # record initial params\n",
    "                initial_params = model.record_params()\n",
    "                params_ls.append(initial_params.copy())\n",
    "                initial_params_summary[network_name] = [initial_params.copy()]\n",
    "                \n",
    "                # Part 1: Random\n",
    "                # random train model w/ opto activation\n",
    "                if random_learning:\n",
    "                    train_type = \"random\"\n",
    "                    opto_on = False\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss_function = \"MSE\"\n",
    "                    print(f'training {network_name}, random-train, with opto, loss:{loss_function}')\n",
    "                    train_model(model, random_train_loader, random_test_loader, random_val_loader, epochs, params_ls, opto_category, \n",
    "                                opto_on, loss_function, grad_clip_value = 4)\n",
    "                    \n",
    "                    # record and store trained params\n",
    "                    trained_params = model.record_params()\n",
    "                    trained_params_summary[network_name] = [trained_params.copy()]\n",
    "                    weight_changes(initial_params, trained_params)\n",
    "\n",
    "                    # store random activations\n",
    "                    random_activations = get_activations(model,random_opto_loader)\n",
    "                    # plot_layer_activation([random_activations], plot = [True])\n",
    "                    \n",
    "\n",
    "\n",
    "                # Before reward/punish training, freeze all layers except LHb\n",
    "                params_to_optimize = [model.LHb.pre_w]\n",
    "                # update adam optimizer\n",
    "                #optimizer = optim.Adam(params = params_to_optimize, lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)\n",
    "                optimizer = LogAdam(params=params_to_optimize)\n",
    "                \n",
    "                \n",
    "                # Part 2: Reward\n",
    "                if reward_learning:\n",
    "                    # record initial params\n",
    "                    initial_params = model.record_params() \n",
    "                    initial_params_summary[network_name].append(initial_params.copy())\n",
    "                    \n",
    "                    # reward-train model w/ opto activation and MSE loss\n",
    "                    train_type = \"reward\"\n",
    "                    opto_on = True\n",
    "                    model.opto_on = opto_on\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss_function = \"MSE\"\n",
    "                    print(f'training {network_name}, reward-train, with opto neurons, loss:{loss_function}')\n",
    "                    train_model(model, reward_train_loader, reward_test_loader, reward_val_loader, epochs, params_ls, opto_category, \n",
    "                                opto_on, loss_function, grad_clip_value = 4)\n",
    "                    \n",
    "                    # record and store trained params\n",
    "                    trained_params = model.record_params()\n",
    "                    trained_params_summary[network_name].append(trained_params.copy())\n",
    "                    weight_changes(initial_params, trained_params)\n",
    "                    \n",
    "                    # store reward activations\n",
    "                    reward_activations = get_activations(model,reward_opto_loader)\n",
    "                \n",
    "                \n",
    "                # Part 3: punish\n",
    "                if punish_learning:\n",
    "                    # record initial params\n",
    "                    initial_params = model.record_params()\n",
    "                    initial_params_summary[network_name].append(initial_params.copy())\n",
    "                    \n",
    "                    # punish-train model w/ opto activation and MSE loss\n",
    "                    train_type = \"punish\"\n",
    "                    opto_on = True\n",
    "                    model.opto_on = opto_on\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss_function = \"MSE\"\n",
    "                    print(f'training {network_name}, punish-train, with opto neurons, loss:{loss_function}')\n",
    "                    train_model(model, punish_train_loader, punish_test_loader, punish_val_loader, epochs, params_ls, opto_category, \n",
    "                                opto_on, loss_function, grad_clip_value = 4)\n",
    "                    \n",
    "                    # record and store trained params\n",
    "                    trained_params = model.record_params()\n",
    "                    trained_params_summary[network_name].append(trained_params.copy())\n",
    "                    weight_changes(initial_params, trained_params)\n",
    "                    \n",
    "                    # store punish activations\n",
    "                    punish_activations = get_activations(model,punish_opto_loader)\n",
    "                \n",
    "                # add to results dictionary - training_loss_summary, test_accuracy_summary\n",
    "                params_summary[network_name] = params_ls\n",
    "                training_loss_summary[network_name] = training_loss\n",
    "                test_accuracy_summary[network_name] = test_accuracy\n",
    "                val_accuracy_summary[network_name] = val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ca52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation values\n",
    "activations = []\n",
    "if random_learning: activations.append(random_activations)\n",
    "if reward_learning: activations.append(reward_activations)\n",
    "if punish_learning: activations.append(punish_activations)\n",
    "\n",
    "plot_layer_activation(activations,plot=[random_learning,reward_learning,punish_learning])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get EP activations and opto indices\n",
    "EP_activations_ls = model.EP_activations_ls\n",
    "opto_idx_trainset = model.opto_idx_trainset\n",
    "\n",
    "# get initial and trained params for random, reward, punish\n",
    "random_idx = 0\n",
    "reward_idx = 1\n",
    "if reward_learning: punish_idx = 2\n",
    "else: punish_idx = 1\n",
    "\n",
    "random_train_initial_params = initial_params_summary['MLP_random_real_corelease_no_split_1 (512 EP neurons, 512 LHb neurons)'][random_idx]\n",
    "random_train_trained_params = trained_params_summary['MLP_random_real_corelease_no_split_1 (512 EP neurons, 512 LHb neurons)'][random_idx]\n",
    "\n",
    "if reward_learning:\n",
    "    reward_train_initial_params = initial_params_summary['MLP_random_real_corelease_no_split_1 (512 EP neurons, 512 LHb neurons)'][reward_idx]\n",
    "    reward_train_trained_params = trained_params_summary['MLP_random_real_corelease_no_split_1 (512 EP neurons, 512 LHb neurons)'][reward_idx]\n",
    "\n",
    "if punish_learning:\n",
    "    punish_train_initial_params = initial_params_summary['MLP_random_real_corelease_no_split_1 (512 EP neurons, 512 LHb neurons)'][punish_idx]\n",
    "    punish_train_trained_params = trained_params_summary['MLP_random_real_corelease_no_split_1 (512 EP neurons, 512 LHb neurons)'][punish_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package into pickle file\n",
    "packed = {\"EP_activations_ls\": model.EP_activations_ls, \"opto_idx_trainset\": model.opto_idx_trainset}\n",
    "\n",
    "packed[\"random_train_initial_params\"] = random_train_initial_params\n",
    "packed[\"random_train_trained_params\"] = random_train_trained_params\n",
    "train_name = \"random\"\n",
    "\n",
    "if reward_learning:\n",
    "    packed[\"reward_train_initial_params\"] = reward_train_initial_params\n",
    "    packed[\"reward_train_trained_params\"] = reward_train_trained_params\n",
    "    train_name += \"-reward\"\n",
    "\n",
    "if punish_learning:\n",
    "    packed[\"punish_train_initial_params\"] = punish_train_initial_params\n",
    "    packed[\"punish_train_trained_params\"] = punish_train_trained_params\n",
    "    train_name += \"-punish\"\n",
    "\n",
    "# Save the list as a pickle file\n",
    "print(f'Saving weights to FMNIST_weights_{train_name}.pkl')\n",
    "with open(f'FMNIST_weights_{train_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(packed, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62609cce",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unpack pickle files\n",
    "# train_name = \"random-reward-punish\"\n",
    "\n",
    "# with open(f'FMNIST_weights_{train_name}.pkl', 'rb') as f:\n",
    "#     FMNIST_weight = pickle.load(f)\n",
    "\n",
    "# # Get EP activations and opto indices\n",
    "# EP_activations_ls = FMNIST_weight['EP_activations_ls']\n",
    "# opto_idx_trainset = FMNIST_weight['opto_idx_trainset']\n",
    "\n",
    "# # Get initial and trained params\n",
    "# random_learning = \"random\" in train_name\n",
    "# reward_learning = \"reward\" in train_name\n",
    "# punish_learning = \"punish\" in train_name\n",
    "\n",
    "# if random_learning:\n",
    "#     random_train_initial_params = FMNIST_weight['random_train_initial_params']\n",
    "#     random_train_trained_params = FMNIST_weight['random_train_trained_params']\n",
    "\n",
    "# if reward_learning:\n",
    "#     reward_train_initial_params = FMNIST_weight['reward_train_initial_params']\n",
    "#     reward_train_trained_params = FMNIST_weight['reward_train_trained_params']\n",
    "    \n",
    "# if punish_learning:\n",
    "#     punish_train_initial_params = FMNIST_weight['punish_train_initial_params']\n",
    "#     punish_train_trained_params = FMNIST_weight['punish_train_trained_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c228d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "LHb_weight = list(random_train_initial_params)[5]\n",
    "opto_neurons = choose_neuron(EP_activations_ls, opto_idx_trainset, 0.05, largest = True)\n",
    "lowest_neurons = choose_neuron(EP_activations_ls, opto_idx_trainset, 0.05, largest = False)\n",
    "\n",
    "# Decide what to plot\n",
    "plot_random = random_learning\n",
    "plot_reward = reward_learning\n",
    "plot_punish = punish_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LHb_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b967c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a tensor with values from 1 to 512\n",
    "all_values = torch.arange(0, 512)\n",
    "non_opto_neurons = all_values[~torch.isin(all_values, torch.tensor(opto_neurons))]\n",
    "\n",
    "# Preallocate the top and bottom activation value lists\n",
    "# Calculate expected sizes if possible\n",
    "num_batches = len(EP_activations_ls)\n",
    "num_opto_neurons = len(opto_neurons)  # Assuming this is known\n",
    "num_lowest_neurons = len(lowest_neurons)\n",
    "\n",
    "top_act_vals = []\n",
    "bottom_act_vals = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    # Get the category indices for the current batch\n",
    "    category_indices = opto_idx_trainset[batch_idx]\n",
    "    \n",
    "    # Gather all opto tensors for the top activations in one go\n",
    "    top_opto_tensors = EP_activations_ls[batch_idx][category_indices][:, opto_neurons]\n",
    "\n",
    "    # Append to top_act_vals\n",
    "    top_act_vals.extend(top_opto_tensors.flatten().tolist())\n",
    "\n",
    "    # Gather bottom activations in one go using filtered values\n",
    "    bottom_opto_tensors = EP_activations_ls[batch_idx][category_indices][:, lowest_neurons]\n",
    "\n",
    "    # Append to bottom_act_vals\n",
    "    bottom_act_vals.extend(bottom_opto_tensors.flatten().tolist())\n",
    "\n",
    "# Convert lists to tensors if needed\n",
    "top_act_vals = torch.tensor(top_act_vals)\n",
    "bottom_act_vals = torch.tensor(bottom_act_vals)\n",
    "\n",
    "top_act_vals = top_act_vals[::4]\n",
    "bottom_act_vals = bottom_act_vals[::50]\n",
    "\n",
    "# Plot a bar chart\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "scatterboxplot([top_act_vals, bottom_act_vals], [\"Top Activation Values\", \"Bottom Activation Values\"], ax)\n",
    "ax.set_title(f\"Average Activation Values during Opto Category: {opto_category}\")\n",
    "ax.set_ylabel(\"Average Activation Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e44228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total positive and negative weight for each neuron\n",
    "def calculate_weights(weight_matrix, pre_neurons):\n",
    "    '''\n",
    "    Weight matrix is of size n_post x n_pre. Therefore, each row represents the weights of a single post-synaptic neuron\n",
    "    Thus this function returns the total positive and negative weights coming from pre_neurons for each postsynaptic neuron\n",
    "    '''\n",
    "    selected_weights = weight_matrix[:, pre_neurons]\n",
    "    positive_weights = torch.sum(selected_weights * (selected_weights > 0), dim=1)\n",
    "    negative_weights = torch.sum(selected_weights * (selected_weights < 0), dim=1)\n",
    "    \n",
    "    return positive_weights.numpy(), negative_weights.numpy()\n",
    "\n",
    "def calculate_EIindex(weight_matrix, pre_neurons):\n",
    "    '''\n",
    "    Weight matrix is of size n_post x n_pre. Therefore, each row represents the weights of a single post-synaptic neuron\n",
    "    Thus this function returns the EI index calculated from the weights coming from pre_neurons for each postsynaptic neuron\n",
    "    '''\n",
    "    selected_weights = weight_matrix[:, pre_neurons]\n",
    "    positive_weights = torch.sum(selected_weights * (selected_weights > 0), dim=1)\n",
    "    negative_weights = -torch.sum(selected_weights * (selected_weights < 0), dim=1)\n",
    "    \n",
    "    return ((negative_weights-positive_weights) / (positive_weights + negative_weights)).numpy()\n",
    "\n",
    "# Calculate weight differences for each synapse in LHb\n",
    "key = 'LHb.final_weight'\n",
    "# top 5% activated neurons\n",
    "# random train\n",
    "random_train_trained_exci_top, random_train_trained_inhi_top = calculate_weights(random_train_trained_params[key], opto_neurons)\n",
    "random_train_trained_EIindex_top = calculate_EIindex(random_train_trained_params[key], opto_neurons)\n",
    "\n",
    "# Caluclate weight differences for each synapse in LHb\n",
    "random_weight_diff_top = (random_train_trained_params[key][:, opto_neurons] - random_train_initial_params[key][:, opto_neurons]).numpy()\n",
    "\n",
    "# bottom 5% activated neurons\n",
    "# random train\n",
    "random_train_trained_exci_bot, random_train_trained_inhi_bot = calculate_weights(random_train_trained_params[key], lowest_neurons)\n",
    "random_train_trained_EIindex_bot = calculate_EIindex(random_train_trained_params[key], lowest_neurons)\n",
    "\n",
    "# Caluclate weight differences for each synapse in LHb\n",
    "random_weight_diff_bot = (random_train_trained_params[key][:, lowest_neurons] - random_train_initial_params[key][:, lowest_neurons]).numpy()\n",
    "\n",
    "# all neurons\n",
    "# random train\n",
    "random_train_trained_exci_all, random_train_trained_inhi_all = calculate_weights(random_train_trained_params[key], torch.arange(random_train_trained_params[key].shape[1]))\n",
    "random_train_trained_EIindex_all = calculate_EIindex(random_train_trained_params[key], torch.arange(random_train_trained_params[key].shape[1]))\n",
    "\n",
    "# Calculate weight differences for each synapse in LHb\n",
    "random_weight_diff_all = (random_train_trained_params[key] - random_train_initial_params[key]).numpy()\n",
    "\n",
    "\n",
    "if reward_learning:\n",
    "    # reward train top 5% activated neurons\n",
    "    reward_train_trained_exci_top, reward_train_trained_inhi_top = calculate_weights(reward_train_trained_params[key], opto_neurons)\n",
    "    reward_train_trained_EIindex_top = calculate_EIindex(reward_train_trained_params[key], opto_neurons)\n",
    "    \n",
    "    # Caluclate weight differences for each synapse in LHb\n",
    "    reward_weight_diff_top = (reward_train_trained_params[key][:, opto_neurons] - reward_train_initial_params[key][:, opto_neurons]).numpy()\n",
    "\n",
    "    # bottom 5% activated neuron\n",
    "    # reward train\n",
    "    reward_train_trained_exci_bot, reward_train_trained_inhi_bot = calculate_weights(reward_train_trained_params[key], lowest_neurons)\n",
    "    reward_train_trained_EIindex_bot = calculate_EIindex(reward_train_trained_params[key], lowest_neurons)\n",
    "\n",
    "    # Caluclate weight differences for each synapse in LHb\n",
    "    reward_weight_diff_bot = (reward_train_trained_params[key][:, lowest_neurons] - reward_train_initial_params[key][:, lowest_neurons]).numpy()\n",
    "    \n",
    "    # all neurons\n",
    "    # reward train\n",
    "    reward_train_trained_exci_all, reward_train_trained_inhi_all = calculate_weights(reward_train_trained_params[key], torch.arange(reward_train_trained_params[key].shape[1]))\n",
    "    reward_train_trained_EIindex_all = calculate_EIindex(reward_train_trained_params[key], torch.arange(reward_train_trained_params[key].shape[1]))\n",
    "    \n",
    "    # Calculate weight differences for each synapse in LHb\n",
    "    reward_weight_diff_all = (reward_train_trained_params[key] - reward_train_initial_params[key]).numpy()\n",
    "\n",
    "if punish_learning:   \n",
    "    # punish train top 5% activated neurons\n",
    "    punish_train_trained_exci_top, punish_train_trained_inhi_top = calculate_weights(punish_train_trained_params[key], opto_neurons)\n",
    "    punish_train_trained_EIindex_top = calculate_EIindex(punish_train_trained_params[key], opto_neurons)\n",
    "\n",
    "    # Caluclate weight differences for each synapse in LHb\n",
    "    punish_weight_diff_top = (punish_train_trained_params[key][:, opto_neurons] - punish_train_initial_params[key][:, opto_neurons]).numpy()\n",
    "\n",
    "    # bottom 5% activated neuron\n",
    "    # punish train\n",
    "    punish_train_trained_exci_bot, punish_train_trained_inhi_bot = calculate_weights(punish_train_trained_params[key], lowest_neurons)\n",
    "    punish_train_trained_EIindex_bot = calculate_EIindex(punish_train_trained_params[key], lowest_neurons)\n",
    "\n",
    "    # Caluclate weight differences for each synapse in LHb\n",
    "    punish_weight_diff_bot = (punish_train_trained_params[key][:, lowest_neurons] - punish_train_initial_params[key][:, lowest_neurons]).numpy()\n",
    "\n",
    "    # all neurons\n",
    "    # punish train\n",
    "    punish_train_trained_exci_all, punish_train_trained_inhi_all = calculate_weights(punish_train_trained_params[key], torch.arange(punish_train_trained_params[key].shape[1]))\n",
    "    punish_train_trained_EIindex_all = calculate_EIindex(punish_train_trained_params[key], torch.arange(punish_train_trained_params[key].shape[1]))\n",
    "\n",
    "    # Calculate weight differences for each synapse in LHb\n",
    "    punish_weight_diff_all = (punish_train_trained_params[key] - punish_train_initial_params[key]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f93291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots side by side\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 7), sharex=False, sharey=False)\n",
    "\n",
    "# Top 5% activated neurons\n",
    "if plot_random: axs[0].scatter(random_train_trained_exci_top, -random_train_trained_inhi_top, label=\"Random\", color=\"lightblue\")\n",
    "if plot_reward: axs[0].scatter(reward_train_trained_exci_top, -reward_train_trained_inhi_top, label=\"Reward\", color=\"blue\")\n",
    "if plot_punish: axs[0].scatter(punish_train_trained_exci_top, -punish_train_trained_inhi_top, label=\"Punish\", color=\"red\")\n",
    "axs[0].set_title(\"Excitatory vs Inhibitory (top 5% activated neurons - 'opto neurons')\")\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel(\"Total excitatory weights\")\n",
    "axs[0].set_ylabel(\"Total inhibitory weights\")\n",
    "x_vals = np.linspace(min(axs[0].get_xlim()[0], axs[0].get_ylim()[0]), max(axs[0].get_xlim()[1], axs[0].get_ylim()[1]), 100)\n",
    "axs[0].plot(x_vals, x_vals, '-', color='gray', alpha=0.3)\n",
    "\n",
    "# Bottom 5% activated neurons\n",
    "if plot_random: axs[1].scatter(random_train_trained_exci_bot, -random_train_trained_inhi_bot, label=\"Random\", color=\"lightblue\")\n",
    "if plot_reward: axs[1].scatter(reward_train_trained_exci_top, -reward_train_trained_inhi_top, label=\"Reward\", color=\"blue\")\n",
    "if plot_punish: axs[1].scatter(punish_train_trained_exci_bot, -punish_train_trained_inhi_bot, label=\"Punish\", color=\"red\")\n",
    "axs[1].set_title(\"Excitatory vs Inhibitory (bottom 5% activated neurons)\")\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel(\"Total excitatory weights\")\n",
    "axs[1].set_ylabel(\"Total inhibitory weights\")\n",
    "x_vals = np.linspace(min(axs[1].get_xlim()[0], axs[1].get_ylim()[0]), max(axs[1].get_xlim()[1], axs[1].get_ylim()[1]), 100)\n",
    "axs[1].plot(x_vals, x_vals, '-', color='gray', alpha=0.3)\n",
    "\n",
    "# All neurons\n",
    "if plot_random: axs[2].scatter(random_train_trained_exci_all, -random_train_trained_inhi_all, label=\"Random\", color=\"lightblue\")\n",
    "if plot_reward: axs[2].scatter(reward_train_trained_exci_all, -reward_train_trained_inhi_all, label=\"Reward\", color=\"blue\")\n",
    "if plot_punish: axs[2].scatter(punish_train_trained_exci_all, -punish_train_trained_inhi_all, label=\"Punish\", color=\"red\")\n",
    "axs[2].set_title(\"Excitatory vs Inhibitory (all neurons)\")\n",
    "axs[2].legend()\n",
    "axs[2].set_xlabel(\"Total excitatory weights\")\n",
    "axs[2].set_ylabel(\"Total inhibitory weights\")\n",
    "x_vals = np.linspace(min(axs[2].get_xlim()[0], axs[2].get_ylim()[0]), max(axs[2].get_xlim()[1], axs[2].get_ylim()[1]), 100)\n",
    "axs[2].plot(x_vals, x_vals, '-', color='gray', alpha=0.3)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Reward_weight_change_scatter.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b9963",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot first 100 neurons' (opto tagged) weights in pre vs post\n",
    "n_row = 6\n",
    "n_col = 4\n",
    "\n",
    "fig, axs = plt.subplots(n_row,n_col,figsize=(30, 40))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i, neuron in enumerate(opto_neurons[:n_row*n_col]):\n",
    "    if plot_random: axs.flatten()[i].hist(random_train_trained_params[LHb_weight][i,opto_neurons], alpha = 0.7, label = \"random\", color = \"lightblue\")\n",
    "    if plot_reward: axs.flatten()[i].hist(reward_train_trained_params[LHb_weight][i,opto_neurons], alpha = 0.7, label = \"Reward\", color = \"blue\")\n",
    "    if plot_punish: axs.flatten()[i].hist(punish_train_trained_params[LHb_weight][i,opto_neurons], alpha = 0.7, label = \"Punish\", color = \"red\")\n",
    "\n",
    "    axs.flatten()[i].set_xlabel('Weights')\n",
    "    axs.flatten()[i].set_title(f'%s (EP neuron %d)' %(LHb_weight, neuron))\n",
    "    axs.flatten()[i].legend()\n",
    "    \n",
    "#plt.savefig('Reward_weight_change.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc14bc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_row = 6\n",
    "n_col = 4\n",
    "fig, axs = plt.subplots(n_row, n_col, figsize=(30, 30))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Use a counter for subplot index\n",
    "for subplot_idx, neuron in enumerate(opto_neurons[:n_row*n_col]):\n",
    "    if plot_random: axs.flatten()[subplot_idx].hist(random_weight_diff_top[neuron], alpha=0.7, label=\"Reward\", color=\"lightblue\")\n",
    "    if plot_reward: axs.flatten()[subplot_idx].hist(reward_weight_diff_top[neuron], alpha=0.7, label=\"Reward\", color=\"blue\")\n",
    "    if plot_punish: axs.flatten()[subplot_idx].hist(punish_weight_diff_top[neuron], alpha=0.7, label=\"Punish\", color=\"red\")\n",
    "\n",
    "    axs.flatten()[subplot_idx].set_xlabel('Weight change')\n",
    "    axs.flatten()[subplot_idx].set_title(f'%s changes (neuron %d)' % (LHb_weight, neuron))\n",
    "    axs.flatten()[subplot_idx].legend()\n",
    "\n",
    "#plt.savefig('Reward_weight_diff_change.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68006d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot EI index distribution\n",
    "\n",
    "# Decide what to plot\n",
    "if plot_random: \n",
    "    data = [random_train_trained_EIindex_top]\n",
    "    labels = [\"Random\"]\n",
    "if reward_learning: \n",
    "    data.append(reward_train_trained_EIindex_top)\n",
    "    labels.append(\"Reward\")\n",
    "if punish_learning: \n",
    "    data.append(punish_train_trained_EIindex_top)\n",
    "    labels.append(\"Punish\")\n",
    "\n",
    "# Plot EI index distribution\n",
    "fig, axs = plt.subplots(1, figsize=(10, 6))\n",
    "scatterboxplot(data, labels, axs, vert=False)\n",
    "\n",
    "# t_stat, p_value = stats.ttest_ind(random_train_trained_EIindex_top, punish_train_trained_EIindex_top)\n",
    "# plt.text(max(max(random_train_trained_EIindex_top), max(punish_train_trained_EIindex_top)) + 0.1, 1.5, f'p = {p_value:.2e}', ha='center', fontsize=12)\n",
    "\n",
    "axs.set_title('EI index')\n",
    "axs.set_xlabel('EI index')\n",
    "axs.set_xlim(-1.1, 1.1)\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('Reward_EI_Index.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EI index distribution\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "random_data = random_train_trained_EIindex_top\n",
    "punish_data = punish_train_trained_EIindex_top\n",
    "axs[0].boxplot([random_data, punish_data], labels=[\"Random\", \"Punish\"], vert=False)\n",
    "for i in range(len(random_data)):\n",
    "    axs[0].plot([random_data[i], punish_data[i]],[1, 2], alpha=0.24, color='gray')\n",
    "\n",
    "axs[0].scatter(random_data, np.ones(len(random_data)) * 1 + np.random.normal(0, 0.02, len(random_data)), color=\"gray\", label=\"Random\", alpha=0.6)\n",
    "axs[0].scatter(punish_data, np.ones(len(punish_data)) * 2 + np.random.normal(0, 0.02, len(punish_data)), color=\"blue\", label=\"Punish\", alpha=0.6)\n",
    "\n",
    "t_stat, p_value = stats.ks_2samp(random_data, punish_data)\n",
    "axs[0].text(max(max(punish_data), max(punish_data)) + 0.1, 1.5, f'p = {p_value:.2}', ha='center', fontsize=12)\n",
    "axs[0].set_title('EI index')\n",
    "axs[0].set_xlabel('EI index')\n",
    "axs[0].set_xlim(-1, 1)\n",
    "\n",
    "\n",
    "random_data = random_train_trained_EIindex_top[random_neurons[:,1]]\n",
    "punish_data = punish_train_trained_EIindex_top[punish_neurons[:,1]]\n",
    "axs[1].boxplot([random_data, punish_data], labels=[\"Random\", \"Punish\"], vert=False)\n",
    "for i in range(len(random_data)):\n",
    "    axs[1].plot([random_data[i], punish_data[i]],[1, 2], alpha=0.24, color='gray')\n",
    "\n",
    "axs[1].scatter(random_data, np.ones(len(random_data)) * 1 + np.random.normal(0, 0.02, len(random_data)), color=\"gray\", label=\"Random\", alpha=0.6)\n",
    "axs[1].scatter(punish_data, np.ones(len(punish_data)) * 2 + np.random.normal(0, 0.02, len(punish_data)), color=\"blue\", label=\"Punish\", alpha=0.6)\n",
    "\n",
    "t_stat, p_value = stats.ks_2samp(random_data, punish_data)\n",
    "axs[1].text(max(max(random_data), max(punish_data)) + 0.1, 1.5, f'p = {p_value:.2}', ha='center', fontsize=12)\n",
    "axs[1].set_title('EI index')\n",
    "axs[1].set_xlabel('EI index')\n",
    "axs[1].set_xlim(-1, 1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Summary of all simulations\n",
    "p_vals = []\n",
    "for sim in range(nboot):\n",
    "    random_data = pre_train_trained_EIindex[random_neurons[:,sim]]\n",
    "    reward_data = post_train_trained_EIindex[reward_neurons[:,sim]]\n",
    "    t_stat, p_value = stats.ks_2samp(random_data, reward_data)\n",
    "    p_vals.append(p_value)\n",
    "axs[2].boxplot(p_vals, vert=False)\n",
    "axs[2].scatter(p_vals, np.ones(len(p_vals)) * 1 + np.random.normal(0, 0.02, len(p_vals)), color=\"blue\", alpha=0.6)\n",
    "axs[2].axvline(x=0.05, color='red', linestyle=':', linewidth=2)\n",
    "axs[2].set_xlabel('Simulated p-value')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5cdc34",
   "metadata": {},
   "source": [
    "## Other useful code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae57a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_mean_means = []\n",
    "\n",
    "for key in model.gradient_history.keys():\n",
    "    gradient_means = []\n",
    "    for i in model.gradient_history[key]:\n",
    "        mean = torch.mean(i)\n",
    "        gradient_means.append(mean)\n",
    "    gradient_mean_means.append(gradient_means)    \n",
    "    plt.plot(gradient_means, alpha = 0.2)\n",
    "df = pd.DataFrame(gradient_mean_means)\n",
    "#mean for each row\n",
    "plt.plot(df.mean().tolist())\n",
    "plt.title(\"Gradient Change During Reward Training for Opto Neurons (Highest Activation)\", y = 1.05)\n",
    "\n",
    "#plt.savefig(\"GradientChangeReward.png\", dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eplhbmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
